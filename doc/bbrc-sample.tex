%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
% \usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}
\setstretch{1.1}

\begin{center}
\begin{huge}Out-Of-Bag Discriminative Graph Mining\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Abstract}
Class-correlated subgraph descriptors are calculated repeatedly on bootstrap samples of a database of molecular graphs, where each database graph is associated with one from a finite set of target classes. 
The associations between descriptors and target classes are recorded over the \emph{out-of-bag} instances according to the well-known out-of-bag estimation approach. However, its application to graph mining is a novelty, since the latter is widely regarded as a computationally intensive task, which lends itself not easily to sampling. 
However, class-correlated, discriminative graph mining is an unstable, supervised process and should therefore be seen in close context to model learning.
The algorithmic implementation presented here uses  the efficient BBRC algorithm for mining the graphs inside each sample. Several bootstrap samples are processed at a time and matched onto the out-of-bag instances.  
A probabilistic chi-square test is performed on the most frequently sampled descriptors (patterns) using a poisson maximum likelihood estimate.
Patterns surviving the test are mapped back on the database molecules, using a parallelized molecular fragment matching method.

\section{Introduction}
A major impediment in data mining is known as selection bias: no training set (sample) is a faithful representation of true state of affairs (the distribution from which the sample was drawn). For the case of discriminative graph mining, this means that there is no way of inferring the exact relationships between chemical fragments and chemical reactivity. Moreover, class-correlated graph mining is an unstable process, where slight changes in the training data lead to quite different results. 
%This is especially true for graph-mining methods that extract only the most correlated fragments from all feasible fragments (sparse selection).
It has been shown that estimates obtained from the out-of-bag instances of a bootstrap sampling process can drastically improve on estimates obtained from a single model, built using the whole training data, in the case of unstable prediction models \cite{breiman96oob}.

In contrast to traditional out-of-bag-estimation, however, the focus of this work is not on estimating the quality of certain model characteristics (such as node class probability), but instead the frequencies of class-correlated subgraphs on the target classes. The latter can be seen as a sort of model statistics, too, in that they are the outputs of supervised selection processes. However, no bagged predictor is built, since graph mining is about descriptor generation, and thus is a preprocessing step to model building.

\section{Related Work}
Method comp. Filter, Embedded (Tsuda). Sampling from POG (Hasan)

\section{Methods}
\subsection{Bootstrapping Graph Databases}
Let $G$ a set of graphs. A \emph{graph database} consists of $G$ and a function $g: G \rightarrow H$, $H \subset \mathbb{N}_0$, mapping graphs to a finite set of \emph{target classes}. Define the class sizes as $h_i=\vert\{G_j \in G \; \vert\; g(G_i)=i\}\vert$, $\forall i \in H$.

Consider a pattern generating process $F: (G,g) \rightarrow (X,k)$, $k=\left(k_1,\ldots,k_{\vert H\vert}\right)$. $X$ are called patterns, $k_i: X \rightarrow \mathbb{N}_0$ are referred to as \emph{pattern support functions} on the target classes.

Run non-parametric bootstrapping, by drawing $n$ samples with replacement from $G$. Ensure that each sample comprises exactly $h_i$ graphs $G$ with $g(G)=i$, drawn with uniform probability $1/h_i$ inside each class $i \in H$ (stratification).

The result of running $F$ on the bootstrap samples is a pattern set $X= X^{(1)}\cup\ldots\cup,X^{(n)}$, and sets of function vectors $k^{(1)},\ldots,k^{(n)}$.

\subsection{Frequencies}
Define
\begin{itemize}
  \item the frequency of support value $j$ on class $i$ as 
    \begin{equation}
        w_{i,j}(x):=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j}\, , \; \forall i \in H, j \in \mathbb{N}_0
    \end{equation}
  \item the frequency of support value $j$ on class $i$, given the sum of support value frequencies equals $h$, as 
    \begin{equation}
      w_{i,j,h}(x):=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j} * \delta_{\sum_{i=1}^{\vert H \vert}k_i^{(l)}(x),\, h}\, , \; \forall i \in H, \; j,h \in \mathbb{N}_0
    \end{equation}
  \item the cumulative frequency of support value $j$ over classes as 
    \begin{equation}
      w_j(x):=\sum_{i \in H}w_{i,j}(x)
    \end{equation}
\end{itemize}
The remainder of this section drops dependency on $x$ for better readability.

\subsection{Probability Distributions}
Consider the finite set of support values $J \subset \mathbb{N}_0$ with $w_j > 0, \; j \in J$, and the categorical distribution for support value $j \in J$ with parameter $W_J:=\sum_{j \in J} w_j$:
\begin{align}
  p(j|W_J) &= w_j/W_J
\end{align}
Consider the \textsc{Poisson} distribution for support value $k$ in class $i$, given the sum of support values equals $j$.
\begin{align}
  p(k\vert\lambda_{i,j}) &= \operatorname{Pois}\left(\lambda_{i,j}\right), \text{where}\\
  \lambda_{i,j}&=\left( \sum_{l \in \mathbb{N}} l*w_{i,l,j} \right) / \sum_{l \in \mathbb{N}} w_{i,l,j}
\end{align}
Parameter $\lambda_{i,j}$ is the sample mean, which is in turn the maximum likelihood estimate of the \textsc{Poisson} distribution.

\subsection{Significance Testing}
\label{ss:significanceTesting}
Consider the probabilistic version of the $\chi^2$ distribution test with Yates correction, defined as
\begin{align}
  \label{align:probX2}
  \chi^2 = \sum_{i \in H}\; \left( \sum_{j \in J}p(j|W_j) \int_0^{\infty}dk\; p(k|\lambda_{i,j}) \frac{(k-E_j(k_i)-0.5)^2}{E_j(k_i)} \right)
\end{align}
where 
\begin{align}
  E_j(k_i) = \frac{j * h_i}{\vert G\vert}
\end{align}

is the expected support value on class $i$ when the sum of support values is $j$. Note that it is derived from $j$, the current support value, and the target class size. Then, $p(\chi^2)=\operatorname{Chi-Square}\left(\vert H\vert-1\right)$, i.e. the degrees of freedom equals the number of target classes minus one.

Consider also the non-probabilistic version, a simplification of equation (\ref{align:probX2}):

\begin{align}
  \label{align:ordX2}
  \chi^2 = \sum_{i \in H} \frac{(\overline{m}-E_i-0.5)^2}{E_i} 
\end{align}

with parameters
\begin{align}
  \overline{m}=\left( \sum_{l \in \mathbb{N}} j*w_{j} \right) / \sum_{l \in \mathbb{N}} w_{j}, \quad E_i = \frac{\overline{m} * h_i}{\vert G\vert}, \quad \overline{m_i}=\lambda_{i,j},
\end{align}
being the sample mean frequency over classes, the expected frequency for class $i$, and the sample mean frequency for class $i$, respectively.


\section{Algorithm}

\subsection{Implementation}
Patterns mined by BBRC are canonical, therefore a hash table can be used to gather results. Here, ``canonical'' means that two isomorphic subgraphs will be always represented by the same pattern string (so called \emph{SMARTS strings}, which describe molecular fragments). The algorithm using $n$ bootstrap samples is shown in Algorithm \ref{alg:bbrc-sample}.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{Estimate pattern significance on out-of-bag instances}
  \label{alg:bbrc-sample}
\begin{algorithmic}[1]
  \Require $dataBase, numBoots, minSamplingSupport, minFrequencyPerSample$
  \State $hash \gets \{\}$
  \For{$i:=1 \to numBoots$} \Comment{Done in parallel}
    \State $res \gets BBRC(drawSample(size(dataBase)), minFrequencyPerSample)$
    \State $insert(hash,res)$
  \EndFor
  \State $ans \gets \emptyset$
  \For{$pattern \in keys(hash)$}
    \If{$length(hash[pattern]) \geq minSamplingSupport$}
    \If{$(p=SignificanceTest(hash[pattern]))>1-\alpha$} \Comment{Test as in section \ref{ss:significanceTesting}}
        \State $ans\gets ans \cup (pattern,p)$
      \EndIf
    \EndIf
  \EndFor
  \Ensure $ans$
\end{algorithmic}
\end{algorithm}

Line 1 creates an initially empty hash table to gather results from BBRC mining in line 3. Importantly, the result $res$ consists of patterns and support values \emph{per class}, i.e. each pattern is used as key in the hash, where the values stored are the class-specific support values. Importantly, these values correspond to the \emph{out-of-bag} instances, not the \emph{in-bag} instances (bootstrap samples). The necessary step of matching the patterns mined from the \emph{in-bag} instances (line 3) on the \emph{out-of-bag} instances is not shown. It is understood to happen inside the BBRC step and implemented using a parallelized molecular fragment matching method. On termination of the loop in line 5, each hash entry has at most $n$ support values per class.

Post-processing the results is very fast and incurs negligible overhead compared to the graph mining step. It consists of removing patterns that (line 8) were not generated often enough by the sampling process (have not enough entries in the hash table), or which (line 9) do not significantly deviate from the overall distribution of classes, as assessed by the probabilistic $\chi^2$ test described in section \ref{ss:significanceTesting}.

For better readability, the listing does not show how the results in $ans$ are used further. They are processed as follows: the caller of Algorithm \ref{alg:bbrc-sample} matches the patterns back onto the graphs of the original database ($G$), which yields an instantiation matrix, with compounds in the rows and patterns (molecular fragments) in the columns. This matching is again done in parallel, and matrix entries can either be of type binary (occurrence vs no occurrence) or frequency (how many times the pattern occurs).

\section{Experiments}
Experiments were conducted to assess the quality of the out-of-bag estimation.

\subsection{Comparison of $p$-Values}
Three methods were compared by their ability to estimate the discriminative potential of patterns, by assessing the deviation between the $p$-values of patterns, as
a) estimated by the respective method, and 
b) obtained by matching the patterns to an independent test set

The methods compared are

\begin{enumerate}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to equation (\ref{align:probX2}). Denote this method by MLE.}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to equation (\ref{align:ordX2}). Denote this method by MEAN.}
  \item{Single runs of the BBRC algorithm. Denote this method by BBRC.} 
\end{enumerate}

The process was repeated 50 times, with 100 bootstrap samples for methods 1 and 2. The whole procedure is described in Algorithm \ref{alg:pValEstimate}.
\begin{algorithm}
  \caption{Estimation of $p$-values}
  \label{alg:pValEstimate}
\begin{algorithmic}[1]
  \Require $graphDatabase, method$ \Comment{method is MLE, MEAN, or BBRC}
  \State $\mathbf{R_1}=\left[ \right]$
  \State $\mathbf{R_2}=\left[ \right]$
  \For{$i:=1 \to 50$}
    \State $[trainSet, testSet] = splitStratified(graphDatabase,0.5)$ \Comment{Split 50:50}
    \State $\left[ \mathbf{patterns}, \mathbf{p^B} \right] = method(trainSet,numBoots=100,\dots)$ \Comment{parameter numBoots ignored for BBRC}
    \State $\mathbf{p'} = match(\mathbf{patterns}, test)$ 
    \State $ \mathbf{R_1} = \left[ \mathbf{R_1}, E_1(\mathbf{p'}, \mathbf{p^B}) \right]$
    \State $ \mathbf{R_2} = \left[ \mathbf{R_2}, E_2(\mathbf{p'}, \mathbf{p^B}) \right]$
  \EndFor
  \Ensure $\mathbf{R_1},\mathbf{R_2}$
\end{algorithmic}
\end{algorithm}

Lines 1 and 2 initialize empty vectors that capture the residuals in estimation. Inside the main loop, a stratified split (i.e. proportions of target classes inside each split equal overall proportions) generates training and test set of equal size. The training set is treated by the selected method, which returns a vector of patterns and a vector of $p$-values, called $\mathbf{p^B}$ (line 5). The patterns are matched on the test set, yielding $p$-values $\mathbf{p'}$ (line 6). Finally, the residual vectors capture the differences between $\mathbf{p^B}$ and  $\mathbf{p'}$ by two different metrics:
\begin{enumerate}
  \item $E1 := \sum_i \Big|\,p^B(i) -p'(i) \,\Big|$
  \item $E2 := \sum_i \left(\, p^B(i) -p'(i) \,\right)^2$
\end{enumerate}

Table \ref{t:anal} details the results.

\input{anal}

\bibliography{bbrc-sample}
\bibliographystyle{plain}

\end{document} 
