%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
% \usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}

\begin{center}
\begin{huge}A Fast Sampling Method for Class-Correlated Subgraphs\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Introduction}
Class-correlated subgraph descriptors are calculated repeatedly on bootstrap samples of a database of molecular graphs, where each graph is associated with one from a finite set of classes.
Each sample is processed by the graph mining webservice architecture. In addition to using the efficient BBRC algorithm for mining the graphs, the server is loaded with several jobs in parallel, which further increases performance.
A probabilistic chi-square test is performed on the most frequently sampled descriptors (patterns) using a poisson MLE estimate.
Patterns suriviving the test are mapped back on the database molecules, using a parallelized molecular fragment matching method.

\section{Methods}
\subsection{Bootstrapping Graph Databases}
Let $G$ a set of graphs. A \emph{graph database} consists of $G$ and a function $g: G \rightarrow H$, $H \subset \mathbb{N}_0$, mapping graphs to a finite set of \emph{target classes}. Define the class sizes as $h_i=\vert\{G_j \in G \; \vert\; g(G_i)=i\}\vert$, $\forall i \in H$.

Consider a pattern generating process $F: (G,g) \rightarrow (X,k)$, $k=\left(k_1,\ldots,k_{\vert H\vert}\right)$. $X$ are called patterns, $k_i: X \rightarrow \mathbb{N}_0$ are referred to as \emph{pattern support functions} on the target classes.

Run non-parametric bootstrapping, by drawing $n$ samples with replacement from $G$. Ensure that each sample comprises exactly $h_i$ graphs $G$ with $g(G)=i$, drawn with uniform probability $1/h_i$ inside each class $i \in H$ (stratification).

The result of running $F$ on the bootstrap samples is a pattern set $X= X^{(1)}\cup\ldots\cup,X^{(n)}$, and sets of function vectors $k^{(1)},\ldots,k^{(n)}$.

\subsection{Frequencies}
Define
\begin{itemize}
  \item the frequency of support value $j$ on class $i$ as 
    \begin{equation}
        w_{i,j}:=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j}\, , \; \forall i \in H, j \in \mathbb{N}_0
    \end{equation}
  \item the frequency of support value $j$ on class $i$, given the sum of support value frequencies equals $h$, as 
    \begin{equation}
      w_{i,j,h}:=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j} * \delta_{\sum_{i=1}^{\vert H \vert}k_i^{(l)}(x),\, h}\, , \; \forall i \in H, \; j,h \in \mathbb{N}_0
    \end{equation}
  \item the cumulative frequency of support value $j$ over classes as 
    \begin{equation}
      w_j(x):=\sum_{i \in H}w_{i,j}(x)
    \end{equation}
\end{itemize}
The remainder of this section drops dependency on $x$ for better readability.

\subsection{Probability Distributions}
Consider the finite set of support values $J \subset \mathbb{N}_0$ with $w_j > 0, \; j \in J$, and the categorical distribution for support value $j \in J$ with parameter $W_J:=\sum_{j \in J} w_j$:
\begin{align}
  p(j|W_J) &= w_j/W_J
\end{align}
Consider the \textsc{Poisson} distribution for support value $k$ in class $i$, given the sum of support values equals $j$.
\begin{align}
  p(k\vert\lambda_{i,j}) &= \operatorname{Pois}\left(\lambda_{i,j}\right), \text{where}\\
  \lambda_{i,j}&=\sum_{l \in \mathbb{N}} l*w_{i,l,j} / \sum_{l \in \mathbb{N}} w_{i,l,j}
\end{align}
Parameter $\lambda_{i,j}$ is the sample mean, which is in turn the MLE of the Poisson distribution.

\subsection{Significance Testing}
\label{ss:significanceTesting}
Consider the probabilistic version of the $\chi^2$ distribution test, defined as
\begin{align}
  \chi^2 = \sum_{i \in H}\; \left( \sum_{j \in J}p(j|W_j) \int_0^{\infty}dk\; p(k|\lambda_{i,j}) \frac{(k-E_j(k_i)^2}{E_j(k_i)} \right)
\end{align}
where 
\begin{align}
  E_j(k_i) = \frac{j * h_i}{\vert G\vert}
\end{align}

the expected support value on class $i$ when the sum of support values is $j$. Note that it is derived from $j$, the current support value, and the target class size. Then, $p(\chi^2)=\operatorname{Chi-Square}\left(\vert H\vert-1\right)$, i.e. the degrees of freedom equals the number of target classes minus one.

\section{Algorithm}

\subsection{Implementation}
Patterns mined by BBRC are canonical, therefore a hash table can be used to gather results. Here, ``canonical'' means that two isomorphic subgraphs will be always represented by the same pattern string (so called \emph{SMARTS strings}, which describe molecular fragments). The algorithm using $n$ bootstrap samples is shown in Algorithm \ref{alg:bbrc-sample}.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{Calculate BBRC descriptors on bootstrap samples of a database}
  \label{alg:bbrc-sample}
\begin{algorithmic}[1]
  \Require $n, minSamplingSupport, minFrequencyPerSample$
  \State $hash \gets \{\}$
  \For{$i:=1 \to n$} \Comment{Done in parallel}
    \State $res \gets BBRC(drawSample(n), minFrequencyPerSample)$
    \State $insert(hash,res)$
  \EndFor
  \State $ans \gets \emptyset$
  \For{$pattern \in keys(hash)$}
    \If{$length(hash[pattern]) \geq minSamplingSupport$}
      \If{$(p=SignificanceTest(hash[pattern]))>1-\alpha$}
        \State $ans\gets ans \cup (pattern,p)$
      \EndIf
    \EndIf
  \EndFor
  \Ensure $ans$
\end{algorithmic}
\end{algorithm}

Line 1 creates an initially empty hash, which gathers results from BBRC mining in line 3. Importantly, the result $res$ consists of patterns and support values \emph{per class}, i.e. each pattern is used as key in the hash, where the values stored are the class-specific support values. On terminate of the loop in line 5, each hash entry has at most $n$ support values per class.

Post-processing the results is very fast compared to the graph mining step. It consists of removing patterns that (line 8) were not often enough generated by the sampling process (have not enough entries in the hash), or which (line 9) do not significantly deviate from the overall distribution of classes, as assessed by the probabilistic $\chi^2$ test described in section \ref{ss:significanceTesting}.

For better readability, the listing does not show how the results in $ans$ are used further. They are processed as follows: the caller of Algorithm \ref{alg:bbrc-sample} matches the patterns back onto the graphs of the original database ($G$), which yields instantiation an instantiation matrix, with the compounds in the rows and patterns (molecular fragments) in the columns. This matching is parallelized, and matrix entries can either be binary (occurrence vs no occurrence) or frequency (how many times the pattern occurs).

\end{document} 
