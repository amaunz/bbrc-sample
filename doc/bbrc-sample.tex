%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
\usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}
\setstretch{1.1}

\begin{center}
\begin{huge}Out-Of-Bag Discriminative Graph Mining\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Abstract}

In class-labeled graph databases, each graph is associated with one from a
finite set of target classes.  This induces associations between subgraphs
occurring in these graphs, and the target classes. The subgraphs with strong
target class associations are called discriminative subgraphs.
In this work, discriminative subgraphs are repeatedly mined on bootstrap
samples of a graph databases in order to estimate
the subgraph associations precisely.  This is done by recording the subgraph frequencies (support values) per class
over the out-of-bag instances of the bootstrap process. 
We investigate two different methods for the approximation of the true
underlying support values from these empirical values, involving
sample mean and maximum likelihood estimation.  We show that both methods
improve the estimation significantly, compared to single runs of discriminative
subgraph mining, by applying the different methods to publicly available,
chemical datasets.
In computational models of toxicology, subgraphs (fragments of chemical structure) are
routinely used to describe toxicological properties of molecules. Apart from
being statistically validated, the sizes of subgraph sets created by the
proposed methods is also much reduced compared to standard methods and may thus
be useful for statistical models, as well as inspection by toxicological
experts.

\section{Introduction}
It is a fundamental fact that no finite sample can be an exact
representation of the unknown underlying distribution from which the sample was
drawn.  For example, in discriminative subgraph mining, there is no way of
inferring the exact correlations between fragments occurring in the graphs and
the target classes, which could serve to identify the set of patterns that
primarily lead to the occurrence of a certain target class, e.g. the relations
between molecular fragments and exhibited toxicity of molecules. Finding such
relations is, however, a major goal in the research of chemical reactivity of
compounds, namely in the area of quantitative structure activity relationships
(QSARs). Graph mining is also used in other areas such as TODO: cite.

Moreover, discriminative graph mining is an unstable process, where slight
changes in the sample lead to very different subgraph sets. Fortunately, this
can be rectified by deliberately, repeatedly disturbing the sampling process
and combining estimations over many samples. For example, in bootstrap
sampling, estimates of generalization error obtained from the out-of-bag
instances were shown to drastically improve on estimates obtained from the
sample as a whole \cite{bylander02estimating, breiman96oob}.  This work employs
the out-of-bag instances to precisely estimate the frequencies (support values)
of the subgraphs on the target classes that govern the training data.
Infrequently occurring subgraphs are filtered out after the bootstrapping.  For
each remaining subgraph, a significance test is run using the estimated support
values to filter out insignificant subgraphs. This results in a statistically
validated, highly significant, discriminative subgraph set.  

In contrast to bagging, combined statistical models are not the subject of this
work.  However, class-correlated subgraph mining is supervised process that
should be seen in close context to model building (see section
\ref{s:relatedWork}). For example, a statistical model could be directly
learned on the subgraph descriptors, incorporating their estimated
support-values. Moreover, human experts should be able to draw conclusions from
the greatly reduced set. Being subgraphs, they can be easily matched on the
chemical compound structure.

The remainder of this work is structured as follows: We present related work
with a focus on discriminative subgraph mining (section \ref{s:relatedWork}),
our proposed methods for estimating support values and $p$-values (section
\ref{s:Methods}), as well as algorithmic implementation
(section \ref{s:Algorithm}). Experiments include validation of support
per class and $p$-values in several methods on publicly available databases of
various sizes and numbers of target classes (section \ref{s:Experiments}),
before we draw conclusion in section \ref{s:Conclusion}.

The contributions of this work are summarized as follows:
\begin{itemize}
  \item Repeated discriminative subgraph mining on bootstrap samples of a
    class-labeled graph database is proposed as a means to stabilize estimates
    of support values per class for the subgraphs, compared to single runs of
    discriminative subgraph mining.  To this end, the subgraphs' distributions
    on the out-of-bag instances associated with different target classes are
    recorded along the bootstrapping process.
  \item Two methods for the estimation of support values from the recorded
    value are described. Both handle multiple classes. Significance tests are
    applied to these distributions and insignificant subgraphs are removed from
    the results.  
  \item The estimated support values, predicted $p$-values, and predicted class
    associations are empirically validated on molecular databases of various
    sizes.  The results indicate significant improvements over estimates
    ordinary discriminative subgraph mining, where the effect is larger the smaller the
    datasets are.  We conclude that, for discriminative graph mining,
    out-of-bag estimation has the potential to generate concise, highly
    discriminative descriptor sets for use in statistical models and/or expert
    inspection of the molecular fragments.
\end{itemize}


\section{Related Work}
\label{s:relatedWork}

Out-of-bag methods have been used to robustly estimate node probabilities and
node error rates in decision trees \cite{breiman96oob} as well as the
generalization error and accuracy of bagged predictors
\cite{bylander02estimating}. In the work by Bylander
\cite{bylander02estimating}, generalization error was well modeled by
out-of-bag estimation, and its small bias could be further improved by a
correction method, where, in order to correct the prediction of a given
instance, similar out-of-bag instances with the same target class were
employed. However, the method of out-of-bag estimation is not confined to these
examples, and may be used to estimate general statistical properties.

Subgraph mining is often employed as a preprocessing step to statistical
learning, because subgraphs may be useful as descriptors
\cite{bringmann10lego,schietgat09mcsfeatures}. To this end, there is a variety
of well-known statistical learning algorithms available ``off the shelf''
\cite{hall09weka,r08language}. This makes a workflow attractive, where
regularities and patterns are extracted from the data, represented in a unified
format and fed into a machine learning algorithm \cite{hkr03molfea}.  However,
subgraph mining often produces huge descriptor sets and experts would not be
able to draw conclusions from the vast amount of very similar subgraphs, since
the few useful patterns would be lost in the flood.  Similarly, the
high-dimensional pattern space would prevent machine learning methods from
obtaining models in acceptable time \cite{Hasan_origami:mining}.  Thus,
post-processing would be required to lower redundancy and eliminate the vast
majority of subgraphs \cite{schietgat09mcsfeatures, CMNK01Frequent, Jun04Spin}. 

Subgraph boosting \cite{saigo09gboost} is an integrated method that employs
subsampling internally, alternating between graph mining and model building.
The method presented here is clearly different from subgraph boosting (and from
boosting in general), because it calculates descriptors that can be used in
wide variety of models afterwards. It is similar in that it outputs a small
collection of most discriminative subgraphs, however, the collection computed
here is much more stable, because it is robust against perturbations of the
dataset.



\section{Methods}
\label{s:Methods}

\subsection{Basic Graph Theory}
\label{ss:BasicGraphTheory}
A graph database is a tuple $(G, \Sigma, a)$, where $G$ is a set of graphs,
$\Sigma \ne \emptyset$  is a totally ordered set of labels and $a: G
\rightarrow C$, $C \subset \mathbb{N}$, is a function that assigns one from a
finite set of class values to every graph in the database.  The set of target
classes $C$ may consist of more than two values.  We consider labeled,
undirected graphs, i.e. a tuples $g=(V,E,\Sigma,\lambda)$, where $V\ne
\emptyset$ is a finite set of nodes and $E \subseteq V = \{\{v_1, v_2\} \in \{V
\times V\}, v_1 \ne v_2\}$ is a set of edges and $\lambda: V\cup E \rightarrow
\Sigma$ is a label function.  We only consider connected graphs here, i.e.
there is a path between each two nodes in the graph.

A subgraph $g'=(V',E',\Sigma',\lambda')$ of $g$ is a graph such that $V'
\subseteq V$ and $E' \subseteq E$ with $V' \ne \emptyset$ and $E' \ne
\emptyset$, $\lambda'(v_1)=\lambda(v_2)$ whenever $v_1=v_2$, and
$\lambda'(e_1)=\lambda(e_2)$ whenever $e_1=e_2$.  If $g'$ is a subgraph of $g$,
we also say that $g'$ covers $g$.  The subset of the database instances $G$
that $g'$ covers is referred to as the occurrences of $g'$, and its size as
support of $g'$.  
As a special case, the size of the subset of occurrences with $a(g)=i$,
for any $g$ in the occurrences, is referred to as the support of $g'$ for class
$i$. Thus, any subgraph has associated support values per class, ranging each between 0 and the support of $g'$.

The subgraphs considered in this work are free subtrees. Here, we define a tree as a graph
with exactly $n-1$ edges that connects its $n$ nodes. A free tree is a tree
without a designated root node. For an introduction to tree mining, see the
overview by Chi \emph{et al.} \cite{CMNK01Frequent}.

\subsection{Formulation of Out-Of-Bag Discriminative Graph Mining}
\label{ss:oob-dgm}

We refer to the subset of a graph database $G$ that contains all the graphs $g$
with $a(g)=i$ as $G^i$.  The procedure first splits the graph database randomly
into equal-sized training and test databases $G_{Train}$ and $G_{Test}$.
Subsequently, stratified bootstrapping is performed on the training data, such
that each sample comprises $\vert G_{Train}^i\vert$ graphs associated with
class $i$, drawn with replacement and uniform probability $1/h_i$ inside each
class $i$.  On average, about 37\% (1/$e$) of training instances (molecules)
will not be drawn in any bootstrap sample (out-of-bag). Subgraph mining mines
the subgraphs on the in-bag instances, but looks up the support values per
class on the out-of-bag instances, by performing subgraph isomorphism tests
(matching).

After the inital split, the bootstrapping is repeated $N$ times, where in each
iteration, pairs of subgraphs and support values per class
$(g,k_1,\ldots,k_{\vert I\vert})$ are produced, meaning that subgraph $g$
occurs in $k_i$ out-of-bag graphs associated with class $i$. The results are
recorded over the $N$ bootstrap samples, such that for each $g$, the list of
support values is a tuple $(\mathbf{k_1}\ldots\mathbf{k_{\vert I\vert}})$,
where $\mathbf{k_i}$ is a vector $(k_i^1\ldots k_i^N)^T$, containing all the
support values.  

The total support is determined from the class specific
support values by summing across target classes: $\mathbf{k}=\sum_{i=1}^{\vert I\vert} \mathbf{k_i}$.
The vector $\mathbf{k}$ contains the (total) support values for each
bootstrap sample, obtained by summing up vectors $\mathbf{k_i}$. 
The vectors $\mathbf{k_i}$ are generally sparse, due to the
instability of discriminative subgraph mining, i.e. perturbations to the
dataset (such as bootstrap sampling) yield almost always a different (but overlapping) selection of
subgraphs. To cope with the variety of rare subgraphs, a fixed threshold removes all subgraphs with less than
$\lfloor0.3*N\rfloor$ entries in the list of support values.

The estimation of support values is performed seperately for each remaining
subgraph, as described in section \ref{s:significance-estimation}. Two methods
are employed, based either on the sample mean support per class, and using data
of the current subgraph only, or on a maximum likelihood estimate, involving
some of the other subgraphs.  Then, a significance test is run on the estimated
support values. 

\subsection{Significance and Support Values Estimation}
\label{s:significance-estimation}
The significance test scenario for each subgraph is described first, followed by the two methods to
compute the input values to the test.

\subsubsection{Significance Test}
\label{ss:significance-test}
% general chisq test
For a given subgraph $g$, we seek a $|I| \times 2$ contingency table that lists the
support per class in the first column and the overall distribution of target
classes in the second column, see Table \ref{t-ContingencyTableIndTest}.
\begin{table}[t]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    ~           &	$g$       & $all$       \\\hline
    class 1	    &	$k_1$     & $|G^1|$     \\\hline
    class 2 	  &	$k_2$     & $|G^2|$     \\\hline
    $\ldots$ 	  &	$\ldots$  & $\ldots$    \\\hline
    class $|I|$	&	$k_{|I|}$ & $|G^{|I|}|$ \\\hline
    $\Sigma$	  &	$k$       & $|G|$       \\\hline
  \end{tabular}
  \caption[]{Contingency table for subgraph $g$.}
  \label{t-ContingencyTableIndTest}
\end{table}
This data allows to check whether $g$'s support values differ
significantly from the overall class distribution.  The $\chi^2_d$ function for
distribution testing, defined as
\begin{equation}
  %\chi^2_d(x,y) = \frac{(y-\frac{xm}{n})^2}{\frac{xm}{n}} + \frac{(x-y-\frac{x(n-m)}{n})^2}{\frac{x(n-m)}{n}},
  \chi^2_d(x,y) = \sum_{i \in \{1,\ldots,|I|\}} \frac{(k_i-E(k_i))^2}{E(k_i)},
  \label{eq:chid}
\end{equation} 
where $E(k_i)=\frac{G^{i}k}{|G|}$ is the expected value of $k_i$, calculates
the sum of squares of deviations from the expected support for all target
classes. The function values are then compared against the $\chi^2$
distribution function to obtain $p$-values and conduct a significance test with
$|I|-1$ degrees of freedom.  The next sections discusses the methods to obtain the $k_i$
entries in the table from the recorded support values (section
\ref{ss:oob-dgm}).  The $|G^{i}|$ values are constants and take the values of
the overall $|G^{i}|$.  This is possible due to the stratified bootstrapping,
which maintains the overall class proportions in each sample (see section
\ref{ss:oob-dgm}).

%% intermediate step
%\subsection{Estimation of Support}
%For each subgraph $g$, the total support is determined from the class specific
%support values by summing across target classes:
%\begin{equation}
%  \mathbf{k}=\sum_{i=1}^{\vert I\vert} \mathbf{k_i}
%  \label{eqn:total-support}
%\end{equation}
%The vector $\mathbf{k}$ contains the (total) support values for $g$ for each
%bootstrap sample, obtained by summing up vectors $\mathbf{k_i}$.  To apply the
%test from section \ref{ss:significance-test}, values for $k_1, \ldots,k_{|I|},k$ 
%must be derived from the vectors
%$\mathbf{k_1},\ldots,\mathbf{k_{|I|}}, \mathbf{k}$.

\subsubsection{Sample Mean Method}
\label{ss:simple-mean}

We set $k_i$ in Table \ref{t-ContingencyTableIndTest} to
$\overline{\mathbf{k_i}}$ for all $i \in \{1,\ldots,|I|\}$, the sample mean
across the entries of vector $\mathbf{k_i}$ (ignoring missing values). The
value $k$ is determined by the $k_i$.

\subsubsection{Maximum Likelihood Estimation Method}
\label{ss:MLE}
%The set of ``surviving'' subgraphs (i.e. the ones remaining after the
%postprocessing step) is rather small compared to the overall set of subgraphs.
%Moreover, the graph mining process mines only subgraphs with a significance
%The graph mining process mines only subgraphs with a significance
%value of $\alpha \le 0.05\%$ on each specific bootstrap, which should yield a very similar
%class distribution, especially for these frequently occurring subgraphs.  Thus,
%we consider the latter largely independent from each other, and, in order to
%improve on simple mean estimation, we use the suspected stability across
%surviving subgraphs and take their support values into account when estimating
%support value of subgraph $g'$. 
Here, we employ some of the other subgraphs to form estimates for the $k_i$.
The first step in this process is to extract the subgraphs with same class bias
as $g$, i.e the class is identified in which $g$ appears more frequently than
expected by comparing $g$'s relative (local) frequencies on the target classes
to the overall (global) relative frequencies of target classes, i.e. on $G$.
Local ties are broken in favor of the dominant global
class. In case of a further tie on the global level, one of the globally
dominant classes is chosen with uniform probability. Given $g$'s class bias,
the other subgraphs with the same class bias (biases determined in the same
manner) are set aside.

In a second step, the subgraphs with same class bias are used to correct $g$'s
local frequencies by weighting. This approach has some similarity to the work
by Bylander \cite{bylander02estimating}, however, his aim is to correct
instance predictions, and his correction employs similar out-of-bag instances,
whereas our correction happens across bootstraps, and on the subgraphs (not
instances) obtained collectively from all the bootstrap samples.  For each
class, we model the event that each $k_i^j \in \mathbf{k_i}$ would occur for
each of the subgraphs with the same class bias as $g$ as a multinomial
selection process.  More specifically, we determine the class probabilities for
each subgraph $g'$ with the same class bias as $g$ with a maximum
likelihood estimator. It is the smoothed vector of relative class specific support
values, defined as:
\begin{equation}
  \mathbf{\alpha_{g'}} = \left(\frac{1+\vert\mathbf{k_1}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert}_1,\ldots,\frac{1+\vert\mathbf{k_{\vert I\vert}}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert_1}\right)
  \label{eqn:mlexpr}
\end{equation}
where the $\mathbf{k_i}$ and $\mathbf{k}$ pertain to $g'$, and $\vert\cdot\vert_1$ is the one-norm (the sum of the vector elements). Following that, for
each tuple $(k_1^j,\ldots,k_{\vert I\vert}^j)$ pertaining to $g$, a probability distribution is
determined from this collection of multinomials:
\begin{equation}
  p((k_1^j,\ldots,k_{\vert I\vert}^j))=\frac{\sum_{g'} p((k_1^j,\ldots,k_{\vert I\vert}^j); \mathbf{\alpha_{g'}})}{\sum_{g'}1}
  \label{eqn:avgpr}
\end{equation}
Finally, the $k_i^j$ values pertaining to $g$ are corrected in a weighted average
based on this probability distribution:
\begin{equation}
  \overline{\mathbf{k_i}}=\frac{\sum_j k_i^j p((k_1^j,\ldots,k_{\vert I\vert}^j))}{\sum_j p((k_1^j,\ldots,k_{\vert I\vert}^j))}
  \label{eqn:avgki}
\end{equation}
Then, equation \ref{eq:chid} is applied to determine the $\chi^2$ value of $g$.

\section{Algorithm}
\label{s:Algorithm}
According to section \ref{ss:oob-dgm}, subgraph mining proceeds in two steps:
mining the bootstrap sample and -- for each subgraph found -- looking up
support values per class on the out-of-bag instances.  The mining step is
implemented using a discriminative subgraph mining algorithm of choice. In this
work, our in-house algorithm backbone refinement class mining (BBRC)
\cite{maunz09largescale} is used.  It has high compression potential, which has
been shown theoretically and empirically, while retaining good database
coverage \cite{maunz11efficient}.  This ensures low running times and small
result set sizes, if the minimum frequency parameter is set appropriately. In
the output, two isomorphic subgraphs will be always represented by the same
string identifier.  We employ SMARTS, a kind of regular expression to encode
molecular fragments as strings.  This allows to store results in a hash
structure using SMARTS as keys.

%When subgraph mining is finished over all bootstrap samples, a post-processing
%step filters out infrequent (threshold from section \ref{ss:oob-dgm}) and
%insignificant subgraphs (as assessed on the basis of the current method, see
%section \ref{s:significance-estimation}). 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[t]
  \caption{Estimate subgraph significance on out-of-bag instances}
  \label{alg:bbrc-sample}
  {\small \begin{algorithmic}[1]
  \Require $dataBase, numBoots, minSamplingSupport, method, minFrequencyPerSample, \alpha$
  \If{$numBoots=1$}
    \State $ans \gets BBRC(dataBase, minFrequencyPerSample, \alpha)$
  \Else
    \State $hash \gets \{\}$
    \For{$i:=1 \to numBoots$} \Comment{Parallel Processing}
        \State $res \gets BBRC(drawBsSample(dataBase), minFrequencyPerSample, \alpha)$
      \State $insert(hash,matchOOB(res))$
    \EndFor
    \State $ans \gets \emptyset$
    \For{$subgraph \in keys(hash)$}
      \If{$length(hash[subgraph]) \geq minSamplingSupport$}
      \If{$(p=SignificanceTest(hash[subgraph]), method) < \alpha$}
          \State $ans\gets ans \cup (subgraph,p)$
        \EndIf
      \EndIf
    \EndFor
  \EndIf
  \Ensure $ans$
\end{algorithmic}}
\end{algorithm}
The algorithm using $numBoots=N$ bootstrap samples is shown in Algorithm
\ref{alg:bbrc-sample}.  We consider the case where $numBoots>1$.  Line 4
creates an initially empty hash table to gather results from BBRC mining in
line 6. The resulting subgraphs $res$ are matched on the out-of-bag instances
in line 7, where the values stored in the hash are the support values per
class. It should be stressed that these values correspond to the out-of-bag
instances, not the in-bag instances (bootstrap samples). The necessary step of
matching the subgraphs on the out-of-bag instances happens inside the BBRC
step. On termination of the loop in line 8, each hash entry has at most $N$
support values per class.  Post-processing the results is very fast and incurs
negligible overhead compared to the graph mining step. It consists of removing
subgraphs that (line 11) were not generated often enough by the sampling
process (have not enough entries in the hash table, as determined by
$minSamplingSupport$), or which (line 12) do not significantly deviate from the
overall distribution of classes, as assessed by the $\chi^2$ test (section
\ref{ss:significance-test}), with contingency table calculated according to
mean (section \ref{ss:simple-mean}) or maximum likelihood estimation (section
\ref{ss:MLE}) method.\\ If $numBoots=1$, support values per class are
obtained directly from a single BBRC run, without any matching.

%For better readability, the listing does not show how the results in $ans$ are
%processed further: The caller of Algorithm \ref{alg:bbrc-sample} matches the
%subgraphs back onto the graphs of the original database ($G$), which yields an
%instantiation matrix, with compounds in the rows and subgraphs (molecular
%fragments) in the columns. This matching is again done in parallel, and matrix
%entries can either be of type binary (occurrence vs no occurrence) or frequency
%(how many times the subgraph occurs).

\section{Experiments}
\label{s:Experiments}
Experiments were conducted to assess the quality of the out-of-bag estimation.

\subsection{Comparison of $p$-Values} Three methods were compared by their
ability to estimate the discriminative potential of subgraphs, by assessing the
deviation between the $p$-values of subgraphs, as a) estimated by the
respective method, and b) obtained by matching the subgraphs to an independent
test set

The methods compared are

\begin{enumerate} 
  \item{Out-of-bag estimation of $p$-values by computing
    chi-square values with Algorithm \ref{alg:bbrc-sample}, according to
    section \ref{ss:MLE}. Denote this method by MLE.} 
  \item{Out-of-bag
    estimation of $p$-values by computing chi-square values with Algorithm
    %\ref{alg:bbrc-sample}, according to section \ref{ss:simple-mean}. Denote
    \ref{alg:bbrc-sample}, according to section . Denote
    this method by MEAN.} 
  \item{Single runs of the BBRC algorithm. Denote this
    method by BBRC.} 
\end{enumerate}

The process was repeated 100 times, with 50 bootstrap samples for methods 1 and 2. The whole procedure is described in Algorithm \ref{alg:pValEstimate}.
\begin{algorithm}
  \caption{Estimation of $p$-values}
  \label{alg:pValEstimate}
  {\small \begin{algorithmic}[1]
  \Require $graphDatabase, method$ \Comment{method is MLE, MEAN, or BBRC}
  \State $\mathbf{E_1}=\mathbf{E_2}=\left[ \right]$
  \For{$i:=1 \to 50$}
    \State $[trainSet, testSet] = splitStratified(graphDatabase,0.5)$ \Comment{Split 50:50}
    \State $\left[ \mathbf{subgraphs}, \mathbf{p^B} \right] = Algorithm \ref{alg:bbrc-sample}(trainSet,numBoots=100,\dots)$ \Comment{$numBoots=1$ for BBRC}
    \State $\mathbf{p'} = match(\mathbf{subgraphs}, test)$ 
    \State $ \mathbf{E_1} = \left[ \mathbf{E_1}, E_1(\mathbf{p'}, \mathbf{p^B}) \right]$
    \State $ \mathbf{E_2} = \left[ \mathbf{E_2}, E_2(\mathbf{p'}, \mathbf{p^B}) \right]$
  \EndFor
  \Ensure $\mathbf{E_1},\mathbf{E_2}$
\end{algorithmic}}
\end{algorithm}

Line 1 initializes empty vectors that capture the residuals in
estimation. Inside the main loop, a stratified split (i.e. proportions of
target classes inside each split equal overall proportions) generates a training
and a test set of equal size. The training set is treated by the selected method,
which returns a vector of subgraphs and a vector of $p$-values, called
$\mathbf{p^B}$ (line 4). The subgraphs are matched on the test set, yielding
$p$-values $\mathbf{p'}$ (line 5). Finally, the residual vectors capture the
differences between $\mathbf{p^B}$ and  $\mathbf{p'}$ by two different metrics:

\begin{enumerate}
  \item $E_1 := \frac{1}{n} \sum_{i=1}^n \,p^B_i -p'_i \,$, to identify bias.
  \item $E_2 := \frac{1}{n} \sum_{i=1}^n \Big|\,p^B_i -p'_i \,\Big|$, to assess accuracy
\end{enumerate}

Table \ref{t:anal} details the results (mean values across bootstraps)
\input{anal}

Table \ref{t:sign} details the results ($n$=100). 
\input{sign}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp1.eps} & \includegraphics[width=5.5cm]{lp1.eps} \\
   \includegraphics[width=10cm]{bp2.eps} & \includegraphics[width=5.5cm]{lp2.eps} \\
   \includegraphics[width=10cm]{bp3.eps} & \includegraphics[width=5.5cm]{lp3.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp13}
\end{figure}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp4.eps} & \includegraphics[width=5.5cm]{lp4.eps} \\
   \includegraphics[width=10cm]{bp5.eps} & \includegraphics[width=5.5cm]{lp5.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp45}
\end{figure}

Figure \ref{fig:bplp13} and Figure \ref{fig:bplp45} plot the results.

\section{Conclusion}
\label{s:Conclusion}


\bibliography{bbrc-sample}
\bibliographystyle{plain}

\end{document} 
