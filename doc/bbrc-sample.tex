%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
\usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}
\setstretch{1.1}

\begin{center}
\begin{huge}Out-Of-Bag Discriminative Graph Mining\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Abstract}
In computational models of toxicology, subgraphs (chemical fragments) and other
descriptors are routinely used to describe toxicological properties of
molecules.  In this work, subgraph descriptors are repeatedly mined on
bootstrap samples of several molecular graph databases, where each graph in a
database is associated with one from a finite set of target classes.  The
associations between descriptors and target classes are recorded over the
out-of-bag instances and used to estimate the number occurrences of each
descriptor in each target class.  We investigate two different schemes for the
generation of significance values from these counts, involving mean values and
maximum likelihood estimation.  We show that both methods improve estimates
greatly, compared to single runs of discriminative subgraph mining in terms of
bias and accuracy, with advantages for the maximum likelihood method.  The
results may be useful for statistical models and inspection by toxicological experts,
since the chemical fragments and their target class associations can be used as
descriptors and are more likely to be interpretable for human experts.

\section{Introduction}
It is a fundamental problem in data mining that no sample is an exact
representation of the unkown underlying distribution from which the sample was
drawn.  For example, in discriminative subgraph mining, there is no way of
inferring the exact correlations between fragments occurring in the graphs and
the target classes, which could serve to identify the set of patterns that primarily lead to the
occurrence of a certain target class. Finding such correlations is however a major goal
in the research of chemical reactivity of compounds, for example (TODO: cite).

Moreover, discriminative graph mining is an unstable process, where slight
changes in the sample lead to very different subgraph sets. Fortunately, this
can be rectified by repeatedly disturbing the sampling process deliberately and
creating estimations from many samples together. For example, in bootstrap
sampling, estimates of generalization error obtained from the out-of-bag
instances were shown to drastically improve on estimates obtained from the sample as a
whole \cite{bylander02estimating, breiman96oob}.

This work employs the out-of-bag instances to estimate the frequencies of the
subgraphs on the target classes that govern the training data. Infrequently
occurring descriptors are filtered out. A statistical test is run on each
remaining descriptor, and its class-specific occurrences, $p$-values, and other
measures, are validated on a test set. Finally, the most predictive descriptors
are output, constituting a statistically validated, highly significant,
discriminative subgraph set.  In contrast to bagging, no combined predictor is
generated.  However, class-correlated subgraph mining is supervised process
that should be seen in close context to model building (see section
\ref{s:relatedWork}). Thus, the generated data set could be regarded, in
analogy to a bagged predictor, as ``bagged descriptor set''. For example, a
statistical model could be directly learned on the descriptors, incorporating
their estimated $p$-values Also, human experts should be able to draw
conlusions from the greatly reduced set. Being subgraphs, they can be easily
matched on the chemical compound structure.

The remainder of this work is structured as follows: We present related work with a focus on graph mining
(section \ref{s:relatedWork}), our proposed methods for estimation of class significance
and algorithmic implementation (sections \ref{s:significance-estimation} and
\ref{s:Algorithm}), and estimations on distribution, class significance, and more using experimental data (section \ref{s:Experiments}), before we draw
conclusion in section \ref{s:Conclusion}.

The contributions of this work are summarized as follows:
\begin{itemize}
  \item Graph mining on bootstrap samples of a class-labeled graph database is
    proposed as a means to stabilize estimates of multinomial distributions,
    i.e. the subgraphs' distribution on the target classes are estimated
    from the out-of-bag instances.
  \item We present two schemes for the estimation step, where both are
    able to handle multiple classes using appropriate significance tests for
    the subgraphs' distributions. 
  \item We validate both schemes with error estimates on the target class
    distributions and $p$-values, as obtained from a separate test set.
    Out-of-bag estimation significantly improves on estimates obtained from the
    whole sample and bias and variance decrease with increasing dataset size.
\end{itemize}


\section{Related Work}
\label{s:relatedWork}

Out-of-bag methods have been used to robustly (re-)estimate node probabilities
and node error rates in decision trees \cite{breiman96oob} as well as the
accuracy \cite{bylander02estimating, breiman96oob} and bias
\cite{bylander02estimating} of the associated bagged predictor.
TODO: MORE MATERIAL

Subgraph boosting \cite{saigo09gboost} is an integrated method that employs
subsampling internally, alternating between graph mining and model building. The method presented here is
clearly different from subgraph boosting (and from boosting in general),
because it calculates descriptors that can be used in wide variety of models
afterwards. It is similar in that it outputs a small collection of most
discriminative subgraphs, however, the collection computed here is much more
stable, because it is robust against perturbations of the dataset.

More often than in integrated methods, however, subgraph mining is employed as
a preprocessing step to statistical learning, because subgraphs may be useful as
descriptors \cite{bringmann10lego,schietgat09mcsfeatures}. To this end, there
is a variety of well-known statistical learning algorithms available ``off the
shelf'' \cite{hall09weka,r08language}. This makes a workflow attractive, where
regularities and patterns are extracted from the data, represented in a unified
format and fed into a machine learning algorithm \cite{hkr03molfea}.  However,
subgraph mining often produces huge descriptor sets and experts would not be
able to draw conclusions from the vast amount of very similar subgraphs, since
the few useful patterns would be lost in the flood. Similarly, the
high-dimensional pattern space would prevent machine learning methods from
obtaining models in acceptable time \cite{Hasan_origami:mining}. Thus,
post-processing is required to lower redundancy and eliminate the vast majority
of descriptors \cite{schietgat09mcsfeatures, CMNK01Frequent, Jun04Spin}. In
the approach presented here, however, small sets of discriminative subgraphs are calculated
already in the main algorithm.



\section{Methods}

\subsection{Basic Graph Theory}
\label{ss:BasicGraphTheory}
%TODO: refer to set theory (reader must be familiar with).\\
%DATABASE
We assume a graph database $G=(\mathbf{g}, \Sigma, a)$, where 
$\mathbf{g}$ is a set of graphs, 
$\Sigma \ne \emptyset$  is a totally ordered set of labels and 
%
%TARGET CLASSES
$a: \mathbf{g} \rightarrow C$, $C \subset \mathbb{N}$, is a function that assigns one from a finite set of \emph{class values} to every graph in the database. 
The set of target classes $C$ may consist of more than two values.
%
%GRAPH
Every graph $g \in \mathbf{g}$ is a labeled, undirected graph, i.e. a tuple $g=(V,E,\Sigma,\lambda)$, where 
$V\ne \emptyset$ is a finite set of nodes and 
$E \subseteq V = \{\{v_1, v_2\} \in \{V \times V\}, v_1 \ne v_2\}$ is a set of edges and 
$\lambda: V\cup E \rightarrow \Sigma$ is a label function. 

%PATH
An ordered set of nodes $\{ v_1, \ldots, v_m\}$ is a path between $v_1$ and $v_m$, if $\{v_i, v_{i+1}\} \in E, i \in \{1,\ldots,m-1\}$ and $v_i \neq v_j$ for all $i,j\in \{1,\ldots,m\}$. 
We only consider connected graphs here, i.e. there is a path between each two nodes in the graph.

%TREE
Assuming a graph with $n$ nodes, then the graph is a tree if it has no more than $n-1$ edges (due to the requirement of connected graphs, it must also have at least $n-1$ edges).
Obviously, any path is a tree, but not \emph{vice versa}.
A rooted tree is an ordinary tree with a designated root node.
If a tree has no root, it is called free tree to emphasize the difference.

%SUBGRAPH
A subgraph $g'=(V',E',\Sigma',\lambda')$ of $g$ is a graph such that 
\begin{itemize}
  \item $V' \subseteq V$ and $E' \subseteq E$ with $V' \ne \emptyset$ and $E' \ne \emptyset$,
  \item $g'$ is connected, and
  \item $\lambda'(v_1)=\lambda(v_2)$ whenever $v_1=v_2$, and $\lambda'(e_1)=\lambda(e_2)$ whenever $e_1=e_2$ for all $v_1 \in V'$, $v_2 \in V$, $e_2 \in E'$, $e_2 \in E$.
\end{itemize}
We say that $g'$ is a subgraph of $g$ and that $g'$ covers $g$. The patterns generated by graph mining in this work are free, connected, subtrees. TODO: MORE MATERIAL
%OCCURRENCES
The subset of $\mathbf{g}$ that $g'$ covers is referred to as the occurrences of $g'$, and its size as support of $g'$ in $\mathbf{g}$, denoted by $supp(g', \mathbf{g})$.


\subsection{Formulation of Out-Of-Bag Discriminative Graph Mining}

A graph database consists of a set of graphs $G$ and a finite set of target
classes $I$, such that any graph is associated with exactly one class. There
may exist more than two classes. We refer to the subset of graphs in $G$ that
are associated with class $i$ as $G^i$.

The procedure for estimating subgraph distributions starts by splitting the graph
database randomly into equal-sized training and test databases ($G_{Train}$ and
$G_{Test}$). Then, bootstrapping is performed on the training data. We run
non-parametric bootstrapping, by drawing $n=\vert G_{Train}\vert$ samples with
replacement from  $G_{Train}$. We use stratified samples, i.e. each sample
comprises exactly $\vert G_{Train}^i\vert$ graphs associated with class $i$, drawn with
uniform probability $1/h_i$ inside each class $i$.

Consider further a subgraph mining process, creating pairs, each consisting of
a subgraph and class specific support $(g,k_1,\ldots,k_{\vert I\vert})$,
meaning that subgraph $g$ occurs $k_i$ times in (a graph with) target class
$i$.  The result of subgraph mining on the $N$ bootstrap samples is recorded,
such that for each $g$, the list of class specific supports is extended, finally
yielding $(g,\mathbf{k_1}\ldots\mathbf{k_{\vert I\vert}})$, where $\mathbf{k_i}$
is a vector $(k_i^1\ldots k_i^N)^T$, containing all the class specific supports
from the $N$ bootstrap samples.
Importantly, the $\mathbf{k_i}$ vectors in general have many missing entries
(\emph{NA} values), due to the instability of discriminative subgraph mining, i.e.
perturbations to the dataset (such as bootstrap sampling) yield in general a
different selection of subgraphs. Thus, we remove the most infrequent subgraphs
from the result set in a postprocessing step. We use a fixed threshold for the number of occurrences in the result set of $\lfloor0.3*N\rfloor$ and keep only subgraphs occurring more often.

On average, about 37\% (1/$e$) of training instances will not be drawn in any
bootstrap sample. These are the out-of-bag instances, on which the estimation
of target class distribution is performed. Two schemes are used for this, where both
perform a chi-squared distribution test for each subgraph, estimating the
deviation of the occurrences of the current subgraph from the global
distribution of target classes. Both schemes calculate an estimated count from
the out-of-bag occurrences obtained over all bootstrap samples. The first one
uses a simple mean over the occurrences of the current subgraph, while the
second one uses a maximum likelihood estimate obtained from some of the other
subgraphs, selected based on their similarity with the current subgraph. The next
section explains the two schemes in more detail.

\section{Significance Estimation}
\label{s:significance-estimation}

For each subgraph $g$, estimation of its class significance is based on the class specific supports on the out-of-bag instances. 
The total support is determined from the class specific supports:

\begin{equation}
  \mathbf{k}=\sum_{i=1}^{\vert I\vert} \mathbf{k_i}
  \label{eqn:total-support}
\end{equation}

The vector $\mathbf{k}$ contains the (total) support values for $g$ for each bootstrap sample.

\subsection{Sample Mean}
\label{ss:simple-mean}

Consider the $\chi^2$ estimate obtained from the sample mean:

\begin{align}
  \chi^2 = \sum_{i \in I} \frac{(\overline{\mathbf{k_i}}-E_i-0.5)^2}{E_i} 
  \label{align:meanX2}
\end{align}

where 

\begin{align}
  E_i=\frac{\overline{\mathbf{k}}*\vert G^i\vert}{\vert G \vert}
\end{align}

is the expected count for class $i$.

\subsection{Maximum Likelihood Estimation}
\label{ss:MLE}

The set of ``surviving'' subgraphs (i.e. the ones remaining after the
postprocessing step) is rather small compared to the overall set of subgraphs.
Moreover, the graph mining process mines only subgraphs with a significance
value of $\alpha \le 0.05\%$ on each specific bootstrap, which should yield a very similar
class distribution, especially for these frequently occurring subgraphs.  Thus,
we consider the latter largely independent from each other, and, in order to
improve on simple mean estimation, we use the suspected stability across
surviving subgraphs and take their support values into account when estimating
support value of subgraph $g$. 

The first step in this process is to extract the subgraphs with same class bias
as $g$, i.e the class is identified in which $g$ appears more frequently than
expected by comparing $g$'s relative (local) frequencies on the target classes
to the overall (global) relative frequencies of target classes (i.e. on $G$).
Note that local and global frequencies are comparable due to the stratified
bootstrapping approach. Local ties are broken in favor of the dominant global
class. In case of a further tie on the global level, one of the globally
dominant classes is chosen with uniform probability. Given $g$'s class bias,
the other subgraphs with the same class bias (biases determined in the same
manner) are set aside.

In a second step, the subgraphs with same class bias are used to correct $g$'s
local frequencies by weighting. This approach has some similarity to the work
by Bylander \cite{bylander02estimating}, however, his aim is to correct
instance predictions, and his correction employs similar out-of-bag instances,
whereas our correction happens across bootstraps, and on the subgraphs (not
instances) obtained collectively from all the bootstrap samples.  For each
class, we model the event that each $k_i^j \in \mathbf{k_i}$ would occur for
each of the subgraphs with the same class bias as $g$ as a multinomial
selection process.  More specifically, we determine the class probabilities for
each subgraph $g'$ with the same class bias as $g$ with a maximum
likelihood estimator. It is the smoothed vector of relative class specific support
values, defined as:
\begin{equation}
  \mathbf{\alpha_{g'}} = \left(\frac{1+\vert\mathbf{k_1}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert}_1,\ldots,\frac{1+\vert\mathbf{k_{\vert I\vert}}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert_1}\right)
  \label{eqn:mlexpr}
\end{equation}
where the $\mathbf{k_i}$ and $\mathbf{k}$ pertain to $g'$, and $\vert\cdot\vert_1$ is the one-norm (the sum of the vector elements). Following that, for
each tuple $(k_1^j,\ldots,k_{\vert I\vert}^j)$ pertaining to $g$, a probability distribution is
determined from this collection of multinomials:
\begin{equation}
  p((k_1^j,\ldots,k_{\vert I\vert}^j))=\frac{\sum_{g'} p((k_1^j,\ldots,k_{\vert I\vert}^j); \mathbf{\alpha_{g'}})}{\sum_{g'}1}
  \label{eqn:avgpr}
\end{equation}
Finally, the $k_i^j$ values pertaining to $g$ are corrected in a weighted average
based on this probability distribution:
\begin{equation}
  \overline{\mathbf{k_i}}=\frac{\sum_j k_i^j p((k_1^j,\ldots,k_{\vert I\vert}^j))}{\sum_j p((k_1^j,\ldots,k_{\vert I\vert}^j))}
  \label{eqn:avgki}
\end{equation}
Then, equation \ref{align:meanX2} is applied to determine the $\chi^2$ value of $g$.

\section{Algorithm}
\label{s:Algorithm}

\subsection{Implementation}
The approach works in two steps: First, the results of the individual
bootstrap samples are collectively stored in a hash. Two isomorphic subgraphs
will be always represented by the same string identifier (\emph{SMARTS}, a kind
of regular expression to describe molecular fragments). Second, a
post-processing step filters out frequent and significant subgraphs. The
algorithm using $numBoots=N$ bootstrap samples, as shown in Algorithm
\ref{alg:bbrc-sample}.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{Estimate subgraph significance on out-of-bag instances}
  \label{alg:bbrc-sample}
\begin{algorithmic}[1]
  \Require $dataBase, numBoots, minSamplingSupport, minFrequencyPerSample$
  \If{$numBoots=1$}
    \State $ans \gets BBRC(dataBase, minFrequencyPerSample)$
  \Else
    \State $hash \gets \{\}$
    \For{$i:=1 \to numBoots$} \Comment{Parallel Processing}
        \State $res \gets BBRC(drawBsSample(dataBase), minFrequencyPerSample)$
      \State $insert(hash,res)$
    \EndFor
    \State $ans \gets \emptyset$
    \For{$subgraph \in keys(hash)$}
      \If{$length(hash[subgraph]) \geq minSamplingSupport$}
      \If{$(p=SignificanceTest(hash[subgraph]))>1-\alpha$}
          \State $ans\gets ans \cup (subgraph,p)$
        \EndIf
      \EndIf
    \EndFor
  \EndIf
  \Ensure $ans$
\end{algorithmic}
\end{algorithm}

We consider the case where $numBoots>1$. Line 4 creates an initially empty hash
table to gather results from BBRC mining in line 6. Importantly, the result
$res$ consists of subgraphs and support values \emph{per class}, i.e. each
subgraph is used as key in the hash, where the values stored are the
class-specific support values. Importantly, these values correspond to the
out-of-bag instances, not the in-bag instances (bootstrap samples). The
necessary step of matching the subgraphs on the out-of-bag instances happens
inside the BBRC step. On termination of the loop in line 8, each hash entry has
at most $N$ support values per class.

Post-processing the results is very fast and incurs negligible overhead
compared to the graph mining step. It consists of removing subgraphs that (line
11) were not generated often enough by the sampling process (have not enough
entries in the hash table, as determined by $minSamplingSupport$), or which
(line 12) do not significantly deviate from the overall distribution of classes,
as assessed by the appropriate $\chi^2$ test variant .

%For better readability, the listing does not show how the results in $ans$ are
%processed further: The caller of Algorithm \ref{alg:bbrc-sample} matches the
%subgraphs back onto the graphs of the original database ($G$), which yields an
%instantiation matrix, with compounds in the rows and subgraphs (molecular
%fragments) in the columns. This matching is again done in parallel, and matrix
%entries can either be of type binary (occurrence vs no occurrence) or frequency
%(how many times the subgraph occurs).

\section{Experiments}
\label{s:Experiments}
Experiments were conducted to assess the quality of the out-of-bag estimation.

\subsection{Comparison of $p$-Values} Three methods were compared by their
ability to estimate the discriminative potential of subgraphs, by assessing the
deviation between the $p$-values of subgraphs, as a) estimated by the
respective method, and b) obtained by matching the subgraphs to an independent
test set

The methods compared are

\begin{enumerate}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to section \ref{ss:MLE}. Denote this method by MLE.}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to section \ref{ss:simple-mean}. Denote this method by MEAN.}
  \item{Single runs of the BBRC algorithm. Denote this method by BBRC.} 
\end{enumerate}

The process was repeated 100 times, with 50 bootstrap samples for methods 1 and 2. The whole procedure is described in Algorithm \ref{alg:pValEstimate}.
\begin{algorithm}
  \caption{Estimation of $p$-values}
  \label{alg:pValEstimate}
\begin{algorithmic}[1]
  \Require $graphDatabase, method$ \Comment{method is MLE, MEAN, or BBRC}
  \State $\mathbf{E_1}=\mathbf{E_2}=\left[ \right]$
  \For{$i:=1 \to 50$}
    \State $[trainSet, testSet] = splitStratified(graphDatabase,0.5)$ \Comment{Split 50:50}
    \State $\left[ \mathbf{subgraphs}, \mathbf{p^B} \right] = Algorithm \ref{alg:bbrc-sample}(trainSet,numBoots=100,\dots)$ \Comment{$numBoots=1$ for BBRC}
    \State $\mathbf{p'} = match(\mathbf{subgraphs}, test)$ 
    \State $ \mathbf{E_1} = \left[ \mathbf{E_1}, E_1(\mathbf{p'}, \mathbf{p^B}) \right]$
    \State $ \mathbf{E_2} = \left[ \mathbf{E_2}, E_2(\mathbf{p'}, \mathbf{p^B}) \right]$
  \EndFor
  \Ensure $\mathbf{E_1},\mathbf{E_2}$
\end{algorithmic}
\end{algorithm}

Line 1 initializes empty vectors that capture the residuals in
estimation. Inside the main loop, a stratified split (i.e. proportions of
target classes inside each split equal overall proportions) generates a training
and a test set of equal size. The training set is treated by the selected method,
which returns a vector of subgraphs and a vector of $p$-values, called
$\mathbf{p^B}$ (line 4). The subgraphs are matched on the test set, yielding
$p$-values $\mathbf{p'}$ (line 5). Finally, the residual vectors capture the
differences between $\mathbf{p^B}$ and  $\mathbf{p'}$ by two different metrics:

\begin{enumerate}
  \item $E_1 := \frac{1}{n} \sum_{i=1}^n \,p^B_i -p'_i \,$, to identify bias.
  \item $E_2 := \frac{1}{n} \sum_{i=1}^n \Big|\,p^B_i -p'_i \,\Big|$, to assess accuracy
\end{enumerate}

Table \ref{t:anal} details the results (mean values across bootstraps)
\input{anal}

Table \ref{t:sign} details the results ($n$=100). 
\input{sign}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp1.eps} & \includegraphics[width=4.4cm]{lp1.eps} \\
   \includegraphics[width=10cm]{bp2.eps} & \includegraphics[width=4.4cm]{lp2.eps} \\
   \includegraphics[width=10cm]{bp3.eps} & \includegraphics[width=4.4cm]{lp3.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp13}
\end{figure}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp4.eps} & \includegraphics[width=4.4cm]{lp4.eps} \\
   \includegraphics[width=10cm]{bp5.eps} & \includegraphics[width=4.4cm]{lp5.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp45}
\end{figure}

Figure \ref{fig:bplp13} and Figure \ref{fig:bplp45} plot the results.

\section{Conclusion}
\label{s:Conclusion}


\bibliography{bbrc-sample}
\bibliographystyle{plain}

\end{document} 
