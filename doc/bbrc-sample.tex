%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
% \usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}
\setstretch{1.1}

\begin{center}
\begin{huge}Out-Of-Bag Discriminative Graph Mining\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Abstract}
Subgraph descriptors are repeatedly mined on bootstrap samples of several databases of molecular graphs, where each graph in a database is associated with one from a finite set of target classes. 
The associations between descriptors and target classes are recorded over the out-of-bag instances and used to estimate significance values of the descriptors. 
We investigate two different schemes for the estimation of significance, involving mean values and a maximum likelihood estimation.
We show that both methods improve greatly on the otherwise unstable process of discriminative subgraph mining in terms of bias and accuracy.

\section{Introduction}
A major impediment in data mining is that no sample is a faithful representation of the distribution from which the sample was drawn. 
For example, in discriminative subgraph mining, there is no way of inferring the exact correlations between chemical fragments and the target classes, which can serve to identify chemical reactivity of compounds that incorporate or not incorporate the fragments, since only finite samples can be processed by a computer. 
Moreover, class-correlated graph mining is an unstable process, where slight changes in the sample lead to very different results. 
Fortunately, estimates obtained from the out-of-bag instances of a bootstrap sampling process can drastically improve on estimates obtained from the whole sample \cite{bylander02estimating, breiman96oob}.

Subgraph mining is often used as a preprocessing step to model building, because class-correlated subgraphs are useful as descriptors for chemical datasets \cite{bringmann10lego,maunz11efficient}.
This work employs the out-of-bag instances to estimate the frequencies of the subgraphs on the target classes and finally combine the most predictive descriptors into a data set. 
In contrast to bagging, no combined predictor is generated in this work, at least not in the strict sense. 
However, class-correlated subgraph mining is supervised process that should be seen in close context to model building. 
Thus, the generated data set could be regarded, in analogy to a bagged predictor, as ``bagged descriptor set''.

\section{Related Work}

Subgraph boosting \cite{saigo09gboost} is an integrated procedure that employs subsampling internally and creates a model. The method presented here is clearly different from subgraph boosting (and from boosting in general), because it calculates descriptors that can be used in wide variety of models afterwards. It is similar in that it outputs a small collection of most discriminative subgraphs, however, the collection computed here is much more stable, because it is robust against perturbations of the dataset.

Out-of-bag methods have been used to robustly (re-)estimate node probabilities and node error rates in decision trees \cite{breiman96oob} as well as the accuracy \cite{bylander02estimating, breiman96oob} and bias \cite{bylander02estimating} of the associated bagged predictor.

\section{Methods}
\subsection{Bootstrapping Graph Databases}
Let $G$ a set of graphs. A \emph{graph database} consists of $G$ and a function $g: G \rightarrow H$, $H \subset \mathbb{N}_0$, mapping graphs to a finite set of \emph{target classes}. Define the class sizes as $h_i=\vert\{G_j \in G \; \vert\; g(G_i)=i\}\vert$, $\forall i \in H$.

Consider a pattern generating process $F: (G,g) \rightarrow (X,k)$, $k=\left(k_1,\ldots,k_{\vert H\vert}\right)$. $X$ are called patterns, $k_i: X \rightarrow \mathbb{N}_0$ are referred to as \emph{pattern support functions} on the target classes.

Run non-parametric bootstrapping, by drawing $n$ samples with replacement from $G$. Ensure that each sample comprises exactly $h_i$ graphs $G$ with $g(G)=i$, drawn with uniform probability $1/h_i$ inside each class $i \in H$ (stratification).

The result of running $F$ on the bootstrap samples is a pattern set $X= X^{(1)}\cup\ldots\cup,X^{(n)}$, and sets of function vectors $k^{(1)},\ldots,k^{(n)}$.

\subsection{Frequencies}
Define
\begin{itemize}
  \item the frequency of support value $j$ on class $i$ as 
    \begin{equation}
        w_{i,j}(x):=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j}\, , \; \forall i \in H, j \in \mathbb{N}_0
    \end{equation}
  \item the frequency of support value $j$ on class $i$, given the sum of support value frequencies equals $h$, as 
    \begin{equation}
      w_{i,j,h}(x):=\sum_{l=1}^n \delta_{k_i^{(l)}(x),\, j} * \delta_{\sum_{i=1}^{\vert H \vert}k_i^{(l)}(x),\, h}\, , \; \forall i \in H, \; j,h \in \mathbb{N}_0
    \end{equation}
  \item the cumulative frequency of support value $j$ over classes as 
    \begin{equation}
      w_j(x):=\sum_{i \in H}w_{i,j}(x)
    \end{equation}
\end{itemize}
The remainder of this section drops dependency on $x$ for better readability.

\subsection{Probability Distributions}
Consider the finite set of support values $J \subset \mathbb{N}_0$ with $w_j > 0, \; j \in J$, and the categorical distribution for support value $j \in J$ with parameter $W_J:=\sum_{j \in J} w_j$:
\begin{align}
  p(j|W_J) &= w_j/W_J
\end{align}
Consider the \textsc{Poisson} distribution for support value $k$ in class $i$, given the sum of support values equals $j$.
\begin{align}
  p(k\vert\lambda_{i,j}) &= \operatorname{Pois}\left(\lambda_{i,j}\right), \text{where}\\
  \lambda_{i,j}&=\left( \sum_{l \in \mathbb{N}} l*w_{i,l,j} \right) / \sum_{l \in \mathbb{N}} w_{i,l,j}
\end{align}
Parameter $\lambda_{i,j}$ is the sample mean, which is in turn the maximum likelihood estimate of the \textsc{Poisson} distribution.

\subsection{Significance Testing}
\label{ss:significanceTesting}
Consider the probabilistic version of the $\chi^2$ distribution test with Yates correction, defined as
\begin{align}
  \label{align:probX2}
  \chi^2 = \sum_{i \in H}\; \left( \sum_{j \in J}p(j|W_j) \int_0^{\infty}dk\; p(k|\lambda_{i,j}) \frac{(k-E_j(k_i)-0.5)^2}{E_j(k_i)} \right)
\end{align}
where 
\begin{align}
  E_j(k_i) = \frac{j * h_i}{\vert G\vert}
\end{align}

is the expected support value on class $i$ when the sum of support values is $j$. Note that it is derived from $j$, the current support value, and the target class size. Then, $p(\chi^2)=\operatorname{Chi-Square}\left(\vert H\vert-1\right)$, i.e. the degrees of freedom equals the number of target classes minus one.

Consider also the non-probabilistic version, a simplification of equation (\ref{align:probX2}):

\begin{align}
  \label{align:ordX2}
  \chi^2 = \sum_{i \in H} \frac{(\overline{m}-E_i-0.5)^2}{E_i} 
\end{align}

with parameters
\begin{align}
  \overline{m}=\left( \sum_{l \in \mathbb{N}} j*w_{j} \right) / \sum_{l \in \mathbb{N}} w_{j}, \quad E_i = \frac{\overline{m} * h_i}{\vert G\vert}, \quad \overline{m_i}=\lambda_{i,j},
\end{align}
being the sample mean frequency over classes, the expected frequency for class $i$, and the sample mean frequency for class $i$, respectively.


\section{Algorithm}

\subsection{Implementation}
Patterns mined by BBRC are canonical, therefore a hash table can be used to gather results. Here, ``canonical'' means that two isomorphic subgraphs will be always represented by the same pattern string (so called \emph{SMARTS strings}, which describe molecular fragments). The algorithm using $n$ bootstrap samples is shown in Algorithm \ref{alg:bbrc-sample}.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{Estimate pattern significance on out-of-bag instances}
  \label{alg:bbrc-sample}
\begin{algorithmic}[1]
  \Require $dataBase, numBoots, minSamplingSupport, minFrequencyPerSample$
  \State $hash \gets \{\}$
  \For{$i:=1 \to numBoots$} \Comment{Done in parallel}
    \State $res \gets BBRC(drawSample(size(dataBase)), minFrequencyPerSample)$
    \State $insert(hash,res)$
  \EndFor
  \State $ans \gets \emptyset$
  \For{$pattern \in keys(hash)$}
    \If{$length(hash[pattern]) \geq minSamplingSupport$}
    \If{$(p=SignificanceTest(hash[pattern]))>1-\alpha$} \Comment{Test as in section \ref{ss:significanceTesting}}
        \State $ans\gets ans \cup (pattern,p)$
      \EndIf
    \EndIf
  \EndFor
  \Ensure $ans$
\end{algorithmic}
\end{algorithm}

Line 1 creates an initially empty hash table to gather results from BBRC mining in line 3. Importantly, the result $res$ consists of patterns and support values \emph{per class}, i.e. each pattern is used as key in the hash, where the values stored are the class-specific support values. Importantly, these values correspond to the \emph{out-of-bag} instances, not the \emph{in-bag} instances (bootstrap samples). The necessary step of matching the patterns mined from the \emph{in-bag} instances (line 3) on the \emph{out-of-bag} instances is not shown. It is understood to happen inside the BBRC step and implemented using a parallelized molecular fragment matching method. On termination of the loop in line 5, each hash entry has at most $n$ support values per class.

Post-processing the results is very fast and incurs negligible overhead compared to the graph mining step. It consists of removing patterns that (line 8) were not generated often enough by the sampling process (have not enough entries in the hash table), or which (line 9) do not significantly deviate from the overall distribution of classes, as assessed by the probabilistic $\chi^2$ test described in section \ref{ss:significanceTesting}.

For better readability, the listing does not show how the results in $ans$ are used further. They are processed as follows: the caller of Algorithm \ref{alg:bbrc-sample} matches the patterns back onto the graphs of the original database ($G$), which yields an instantiation matrix, with compounds in the rows and patterns (molecular fragments) in the columns. This matching is again done in parallel, and matrix entries can either be of type binary (occurrence vs no occurrence) or frequency (how many times the pattern occurs).

\section{Experiments}
Experiments were conducted to assess the quality of the out-of-bag estimation.

\subsection{Comparison of $p$-Values}
Three methods were compared by their ability to estimate the discriminative potential of patterns, by assessing the deviation between the $p$-values of patterns, as
a) estimated by the respective method, and 
b) obtained by matching the patterns to an independent test set

The methods compared are

\begin{enumerate}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to equation (\ref{align:probX2}). Denote this method by MLE.}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to equation (\ref{align:ordX2}). Denote this method by MEAN.}
  \item{Single runs of the BBRC algorithm. Denote this method by BBRC.} 
\end{enumerate}

The process was repeated 50 times, with 100 bootstrap samples for methods 1 and 2. The whole procedure is described in Algorithm \ref{alg:pValEstimate}.
\begin{algorithm}
  \caption{Estimation of $p$-values}
  \label{alg:pValEstimate}
\begin{algorithmic}[1]
  \Require $graphDatabase, method$ \Comment{method is MLE, MEAN, or BBRC}
  \State $\mathbf{R_1}=\left[ \right]$
  \State $\mathbf{R_2}=\left[ \right]$
  \For{$i:=1 \to 50$}
    \State $[trainSet, testSet] = splitStratified(graphDatabase,0.5)$ \Comment{Split 50:50}
    \State $\left[ \mathbf{patterns}, \mathbf{p^B} \right] = method(trainSet,numBoots=100,\dots)$ \Comment{parameter numBoots ignored for BBRC}
    \State $\mathbf{p'} = match(\mathbf{patterns}, test)$ 
    \State $ \mathbf{R_1} = \left[ \mathbf{R_1}, E_1(\mathbf{p'}, \mathbf{p^B}) \right]$
    \State $ \mathbf{R_2} = \left[ \mathbf{R_2}, E_2(\mathbf{p'}, \mathbf{p^B}) \right]$
  \EndFor
  \Ensure $\mathbf{R_1},\mathbf{R_2}$
\end{algorithmic}
\end{algorithm}

Lines 1 and 2 initialize empty vectors that capture the residuals in estimation. Inside the main loop, a stratified split (i.e. proportions of target classes inside each split equal overall proportions) generates training and test set of equal size. The training set is treated by the selected method, which returns a vector of patterns and a vector of $p$-values, called $\mathbf{p^B}$ (line 5). The patterns are matched on the test set, yielding $p$-values $\mathbf{p'}$ (line 6). Finally, the residual vectors capture the differences between $\mathbf{p^B}$ and  $\mathbf{p'}$ by two different metrics:
\begin{enumerate}
  \item $E1 := \sum_i \Big|\,p^B(i) -p'(i) \,\Big|$
  \item $E2 := \sum_i \left(\, p^B(i) -p'(i) \,\right)^2$
\end{enumerate}

Table \ref{t:anal} details the results.

\input{anal}

\bibliography{bbrc-sample}
\bibliographystyle{plain}

\end{document} 
