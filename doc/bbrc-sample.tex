%\documentclass[a4paper,10pt]{article}
\documentclass{article}

\usepackage{a4wide}
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}

% % % Watermark
\usepackage{eso-pic}
\usepackage{type1cm}

% % % Figures
% \usepackage{listings}
\usepackage{multirow}	% for tables
\usepackage{graphicx}   % for including EPS
\usepackage{rotating}
\usepackage{subfigure}

% % % Special mathematical fonts
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{srctex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{setspace}

\makeindex
\makeatletter

% --- FORMAT ---------------------------------------------------------

% % % Page Style
% Lamport, L., LaTeX : A Documentation Preparation System User's Guide and Reference Manual, Addison-Wesley Pub Co., 2nd edition, August 1994.
\topmargin -2.0cm        % s. Lamport p.163
\oddsidemargin -0.5cm   % s. Lamport p.163
\evensidemargin -0.5cm  % wie oddsidemargin aber fr left-hand pages
\textwidth 17.5cm
\textheight 22.94cm 
\parskip 7.2pt           % spacing zwischen paragraphen
% \renewcommand{\baselinestretch}{2}\normalsize
\parindent 0pt		 % Einrcken von Paragraphen
\headheight 14pt
\pagestyle{fancy}
\lhead{}
\chead{\bfseries}
\rhead{\thepage}
\lfoot{}
\cfoot{}
\rfoot{}
\renewcommand{\textfloatsep}{1.5em}

% % % Proofs: QED-Box
\renewenvironment{proof}[1][\proofname]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{:}]\ignorespaces
}{
  \popQED\endtrivlist\@endpefalse
}
\makeatother

% % % Alphabetic footnote marks
\renewcommand{\thefootnote}{\alph{footnote}}

% % % Watermark
% \makeatletter
% \AddToShipoutPicture{%
% \setlength{\@tempdimb}{.5\paperwidth}%
% \setlength{\@tempdimc}{.5\paperheight}%
% \setlength{\unitlength}{1pt}%
% \makebox(960,1470){%
% \rotatebox{315}{%
% \textcolor[gray]{0.75}{%
% \fontsize{36pt}{72pt}\selectfont{\emph{D R A F T}}}}}
% }
% \makeatother



% --- START DOCUMENT -------------------------------------------------

\begin{document}
\setstretch{1.1}

\begin{center}
\begin{huge}Out-Of-Bag Discriminative Graph Mining\end{huge}

Andreas Maunz \\Institute for Physics, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
\end{center}

\section{Abstract}

In class-labeled graph databases, each graph is associated with one from a
finite set of target classes.  This induces associations between subgraphs
occurring in these graphs, and the target classes. The subgraphs with strong
target class associations are deemed interesting.
In this work, such discriminative subgraphs are repeatedly mined on bootstrap
samples of several databases in order to estimate
the subgraph associations precisely.  This is done by recording supports per class
over the out-of-bag instances of the bootstrap process. 
We investigate two different schemes for the approximation of the true
underlying supports from these empirical support values, involving
sample mean and maximum likelihood estimation.  We show that both methods
improve the process significantly, compared to single runs of discriminative
subgraph mining by applying the different methods to publicly available,
chemical datasets.
In computational models of toxicology, fragments of chemical structures are
routinely used to describe toxicological properties of molecules. Apart from
being statistically validated, the sizes of subgraph sets created by the
proposed methods is also much reduced compared to standard methods and may thus
be useful for statistical models, as well as inspection by toxicological
experts.

\section{Introduction}
It is a fundamental problem in data mining that no sample is an exact
representation of the unknown underlying distribution from which the sample was
drawn.  For example, in discriminative subgraph mining, there is no way of
inferring the exact correlations between fragments occurring in the graphs and
the target classes, which could serve to identify the set of patterns that
primarily lead to the occurrence of a certain target class, e.g. the relations
between molecular fragments and exhibited toxicity of molecules. Finding such
relations is, however, a major goal in the research of chemical reactivity of
compounds, namely in the area of quantitative structure activity relationships
(QSARs) TODO: cite.

Moreover, discriminative graph mining is an unstable process, where slight
changes in the sample lead to very different subgraph sets. Fortunately, this
can be rectified by deliberately, repeatedly disturbing the sampling process
and combining estimations over many samples. For example, in bootstrap
sampling, estimates of generalization error obtained from the out-of-bag
instances were shown to drastically improve on estimates obtained from the
sample as a whole \cite{bylander02estimating, breiman96oob}.

This work employs the out-of-bag instances to estimate the frequencies
(support) of the subgraphs on the target classes that govern the training data.
Infrequently occurring descriptors are filtered out after the bootstrapping.
For each remaining subgraph, a significance test is run using the estimated supports, creating a $p$-value. This results in a statistically
validated, highly significant, discriminative subgraph set.  In contrast to
bagging, no combined predictor is generated.  However, class-correlated
subgraph mining is supervised process that should be seen in close context to
model building (see section \ref{s:relatedWork}). Thus, the generated data set
could be regarded, in analogy to a bagged predictor, as ``bagged descriptor
set''. For example, a statistical model could be directly learned on the
descriptors, incorporating their estimated $p$-values Also, human experts
should be able to draw conclusions from the greatly reduced set. Being
subgraphs, they can be easily matched on the chemical compound structure.

The remainder of this work is structured as follows: We present related work
with a focus on discriminative subgraph mining (section \ref{s:relatedWork}),
our proposed methods for estimating supports and $p$-values (section
\ref{s:significance-estimation}), as well as algorithmic implementation
(section \ref{s:Algorithm}). Experiments include validation of support
per class and $p$-values in several variants on publicly available databases of
various sizes and numbers of target classes (section \ref{s:Experiments}),
before we draw conclusion in section \ref{s:Conclusion}.

The contributions of this work are summarized as follows:
\begin{itemize}
  \item Graph mining on bootstrap samples of a class-labeled graph database is
    proposed as a means to stabilize estimates of supports per class,
    i.e. the subgraphs' distributions on the target classes are recorded over
    the out-of-bag instances.  
  \item We present two schemes for the estimation of supports. Both handle
    multiple classes. We apply significance tests to these distributions and
    accordingly remove insignificant subgraphs from the results.  
  \item Experimentally, the estimated supports, predicted $p$-values, and
    predicted class associations are validated on molecular databases of
    several sizes, employing training and test sets of equal size per dataset.
    The results indicate significant improvements over estimates obtained from the
    whole sample, where the effect is larger the smaller the datasets are.  We
    conclude that, for discriminative graph mining, out-of-bag estimation has
    the potential to generate concise, highly discriminative descriptor sets
    for use in statistical models and/or expert inspection of the molecular fragments.
\end{itemize}


\section{Related Work}
\label{s:relatedWork}

Out-of-bag methods have been used to robustly (re-)estimate node probabilities
and node error rates in decision trees \cite{breiman96oob} as well as the
generalization error and accuracy of bagged predictors
\cite{bylander02estimating}. In the work by Bylander
\cite{bylander02estimating}, generalization error was well modeled by
out-of-bag estimation, and its small bias could be further improved by a correction method,
where, in order to correct the prediction of a given instance, similar
out-of-bag instances with the same target class were employed.

Subgraph boosting \cite{saigo09gboost} is an integrated method that employs
subsampling internally, alternating between graph mining and model building. The method presented here is
clearly different from subgraph boosting (and from boosting in general),
because it calculates descriptors that can be used in wide variety of models
afterwards. It is similar in that it outputs a small collection of most
discriminative subgraphs, however, the collection computed here is much more
stable, because it is robust against perturbations of the dataset.

More often than in integrated methods, however, subgraph mining is employed as
a preprocessing step to statistical learning, because subgraphs may be useful as
descriptors \cite{bringmann10lego,schietgat09mcsfeatures}. To this end, there
is a variety of well-known statistical learning algorithms available ``off the
shelf'' \cite{hall09weka,r08language}. This makes a workflow attractive, where
regularities and patterns are extracted from the data, represented in a unified
format and fed into a machine learning algorithm \cite{hkr03molfea}.  However,
subgraph mining often produces huge descriptor sets and experts would not be
able to draw conclusions from the vast amount of very similar subgraphs, since
the few useful patterns would be lost in the flood. Similarly, the
high-dimensional pattern space would prevent machine learning methods from
obtaining models in acceptable time \cite{Hasan_origami:mining}. Thus,
post-processing is required to lower redundancy and eliminate the vast majority
of descriptors \cite{schietgat09mcsfeatures, CMNK01Frequent, Jun04Spin}. In
the approach presented here, however, small sets of discriminative subgraphs are calculated
already in the main algorithm.





\section{Methods}

\subsection{Basic Graph Theory}
\label{ss:BasicGraphTheory}
A graph database is a tuple $(G, \Sigma, a)$, where $G$ is a set of graphs,
$\Sigma \ne \emptyset$  is a totally ordered set of labels and $a: G
\rightarrow C$, $C \subset \mathbb{N}$, is a function that assigns one from a
finite set of class values to every graph in the database.  The set of target
classes $C$ may consist of more than two values.  We consider labeled,
undirected graphs, i.e. a tuples $g=(V,E,\Sigma,\lambda)$, where $V\ne
\emptyset$ is a finite set of nodes and $E \subseteq V = \{\{v_1, v_2\} \in \{V
\times V\}, v_1 \ne v_2\}$ is a set of edges and $\lambda: V\cup E \rightarrow
\Sigma$ is a label function.  We only consider connected graphs here, i.e.
there is a path between each two nodes in the graph.

A subgraph $g'=(V',E',\Sigma',\lambda')$ of $g$ is a graph such that $V'
\subseteq V$ and $E' \subseteq E$ with $V' \ne \emptyset$ and $E' \ne
\emptyset$, $\lambda'(v_1)=\lambda(v_2)$ whenever $v_1=v_2$, and
$\lambda'(e_1)=\lambda(e_2)$ whenever $e_1=e_2$.  If $g'$ is a subgraph of $g$,
we also say that $g'$ covers $g$.  The subset of the database instances $G$
that $g'$ covers is referred to as the occurrences of $g'$, and its size as
support of $g'$.  As a special case, the subset of occurrences with $a(g)=i$,
for all $g$ in the occurrences of $g'$, is referred to as the 
occurrences of $g$ for class $i$, its size is the support of $g'$ for class
$i$ (supports per class).

The subgraphs in this work are free subtrees. Here, we define a tree as a graph
with exactly $n-1$ edges that connects its $n$ nodes. A free tree is a tree
without a designated root node. For an introduction to tree mining, see the
overview by Chi \emph{et al.} \cite{CMNK01Frequent}.

\subsection{Formulation of Out-Of-Bag Discriminative Graph Mining}
\label{ss:oob-dgm}

We refer to the subset of graphs in $G$ associated with class $i$ as $G^i$.
The procedure first splits the graph database randomly into equal-sized
training and test databases $G_{Train}$ and $G_{Test}$. Subsequently,
stratified bootstrapping is performed on the training data, such that each
sample comprises $\vert G_{Train}^i\vert$ graphs associated with class $i$,
drawn with replacement and uniform probability $1/h_i$ inside each class $i$.

On average, about 37\% (1/$e$) of training instances (molecules) will not be
drawn in any bootstrap sample (out-of-bag). Subgraph mining
mines the subgraphs on the in-bag instances, but returns the supports per class
on the out-of-bag instances.  

The bootstrapping is repeated $N$ times, where in each iteration, pairs of
subgraphs and supports per class $(g,k_1,\ldots,k_{\vert I\vert})$, meaning
that subgraph $g$ occurs $k_i$ times in a graph $g'$ with $a(g')=i$, are
produced.  The results are recorded over the $N$ bootstrap samples, such that
for each $g$, the list of supports is a tuple
$(\mathbf{k_1}\ldots\mathbf{k_{\vert I\vert}})$, where $\mathbf{k_i}$ is a
vector $(k_i^1\ldots k_i^N)^T$, containing all the supports.  The vectors
$\mathbf{k_i}$ are generally sparse, due to the instability of discriminative
subgraph mining, because perturbations to the dataset (such as bootstrap
sampling) yield mostly a different selection of subgraphs. Thus, a fixed
threshold removes all subgraphs with less than $\lceil0.7*N\rceil$ entries.

Two schemes are employed: Both estimate supports per class for each subgraph,
but differently: either via the sample mean support per class over the
bootstrap samples (using data of the current subgraph only), or via  a maximum
likelihood estimate involving some of the other subgraphs.  Then, a
significance test is run on the estimated supports.  The next section explains
the two schemes in more detail.

\section{Class Support and Significance Test}
\label{s:significance-estimation}

\subsection{Significance Test}
\label{ss:significance-test}
% general chisq test
For a given subgraph $g$, a $|I| \times 2$ contingency table lists the
support per class in the first column, i.e. depending on whether the
ocurrences $G' \subseteq G$ covered by $g$ have the appropriate class value, i.e. $a(g')$, $g' \in G'$.
Moreover, the contingency table contains the overall distribution of target
classes in the second column.  The result is depicted in Table
\ref{t-ContingencyTableIndTest}.

\begin{table}[t]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline
    ~           &	$g$       & $all$       \\\hline
    class 1	    &	$k_1$     & $|G^1|$     \\\hline
    class 2 	  &	$k_2$     & $|G^2|$     \\\hline
    $\ldots$ 	  &	$\ldots$  & $\ldots$    \\\hline
    class $|I|$	&	$k_{|I|}$ & $|G^{|I|}|$ \\\hline
    $\Sigma$	  &	$k$       & $|G|$       \\\hline
  \end{tabular}
  \caption[]{Contingency table for subgraph $g$.}
  \label{t-ContingencyTableIndTest}
\end{table}

It can now be checked whether the $g$'s supports differ
significantly from the overall class distribution.  The $\chi^2_d$ function for
distribution testing, defined as
\begin{equation}
  %\chi^2_d(x,y) = \frac{(y-\frac{xm}{n})^2}{\frac{xm}{n}} + \frac{(x-y-\frac{x(n-m)}{n})^2}{\frac{x(n-m)}{n}},
  \chi^2_d(x,y) = \sum_{i \in \{1,\ldots,|I|\}} \frac{(k_i-E(k_i))^2}{E(k_i)},
  \label{eq:chid}
\end{equation} 
where $E(k_i)=\frac{G^{i}k}{|G|}$ is the expected value of $k_i$, calculates
the sum of squares of deviations from the expected support for all target classes. 

% intermediate step
\subsection{Estimation of Support}
For each subgraph $g$, the total support is determined from the class specific
support values by summing across target classes:
\begin{equation}
  \mathbf{k}=\sum_{i=1}^{\vert I\vert} \mathbf{k_i}
  \label{eqn:total-support}
\end{equation}
The vector $\mathbf{k}$ contains the (total) support values for $g$ for each
bootstrap sample, obtained by summing up vectors $\mathbf{k_i}$.  To apply the
test from section \ref{ss:significance-test}, values for $k_1, \ldots,k_{|I|},k$ 
must be derived from the vectors
$\mathbf{k_1},\ldots,\mathbf{k_{|I|}}, \mathbf{k}$.

\subsection{Sample Mean}
\label{ss:simple-mean}

Consider the $\chi^2$ estimate obtained from the sample mean:

\begin{align}
  \chi^2 = \sum_{i \in I} \frac{(\overline{\mathbf{k_i}}-E_i-0.5)^2}{E_i} 
  \label{align:meanX2}
\end{align}

where 

\begin{align}
  E_i=\frac{\overline{\mathbf{k}}*\vert G^i\vert}{\vert G \vert}
\end{align}

is the expected count for class $i$.

\subsection{Maximum Likelihood Estimation}
\label{ss:MLE}

The set of ``surviving'' subgraphs (i.e. the ones remaining after the
postprocessing step) is rather small compared to the overall set of subgraphs.
Moreover, the graph mining process mines only subgraphs with a significance
value of $\alpha \le 0.05\%$ on each specific bootstrap, which should yield a very similar
class distribution, especially for these frequently occurring subgraphs.  Thus,
we consider the latter largely independent from each other, and, in order to
improve on simple mean estimation, we use the suspected stability across
surviving subgraphs and take their support values into account when estimating
support value of subgraph $g$. 

The first step in this process is to extract the subgraphs with same class bias
as $g$, i.e the class is identified in which $g$ appears more frequently than
expected by comparing $g$'s relative (local) frequencies on the target classes
to the overall (global) relative frequencies of target classes (i.e. on $G$).
Note that local and global frequencies are comparable due to the stratified
bootstrapping approach. Local ties are broken in favor of the dominant global
class. In case of a further tie on the global level, one of the globally
dominant classes is chosen with uniform probability. Given $g$'s class bias,
the other subgraphs with the same class bias (biases determined in the same
manner) are set aside.

In a second step, the subgraphs with same class bias are used to correct $g$'s
local frequencies by weighting. This approach has some similarity to the work
by Bylander \cite{bylander02estimating}, however, his aim is to correct
instance predictions, and his correction employs similar out-of-bag instances,
whereas our correction happens across bootstraps, and on the subgraphs (not
instances) obtained collectively from all the bootstrap samples.  For each
class, we model the event that each $k_i^j \in \mathbf{k_i}$ would occur for
each of the subgraphs with the same class bias as $g$ as a multinomial
selection process.  More specifically, we determine the class probabilities for
each subgraph $g'$ with the same class bias as $g$ with a maximum
likelihood estimator. It is the smoothed vector of relative class specific support
values, defined as:
\begin{equation}
  \mathbf{\alpha_{g'}} = \left(\frac{1+\vert\mathbf{k_1}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert}_1,\ldots,\frac{1+\vert\mathbf{k_{\vert I\vert}}\vert_1}{\vert I\vert+\vert\mathbf{k}\vert_1}\right)
  \label{eqn:mlexpr}
\end{equation}
where the $\mathbf{k_i}$ and $\mathbf{k}$ pertain to $g'$, and $\vert\cdot\vert_1$ is the one-norm (the sum of the vector elements). Following that, for
each tuple $(k_1^j,\ldots,k_{\vert I\vert}^j)$ pertaining to $g$, a probability distribution is
determined from this collection of multinomials:
\begin{equation}
  p((k_1^j,\ldots,k_{\vert I\vert}^j))=\frac{\sum_{g'} p((k_1^j,\ldots,k_{\vert I\vert}^j); \mathbf{\alpha_{g'}})}{\sum_{g'}1}
  \label{eqn:avgpr}
\end{equation}
Finally, the $k_i^j$ values pertaining to $g$ are corrected in a weighted average
based on this probability distribution:
\begin{equation}
  \overline{\mathbf{k_i}}=\frac{\sum_j k_i^j p((k_1^j,\ldots,k_{\vert I\vert}^j))}{\sum_j p((k_1^j,\ldots,k_{\vert I\vert}^j))}
  \label{eqn:avgki}
\end{equation}
Then, equation \ref{align:meanX2} is applied to determine the $\chi^2$ value of $g$.

\section{Algorithm}
\label{s:Algorithm}

\subsection{Implementation}
\label{ss:Implementation}
According to section \ref{ss:oob-dgm}, subgraph mining proceeds in two steps:
mining the bootstrap sample and -- for each subgraph found -- looking up
supports per class on the out-of-bag instances.

The mining step is implemented using the approach of Backbone Refinement Class
Mining (BBRC) \cite{maunz09largescale}, which mines frequent and significantly
correlated subgraphs from graph databases. It has itself a high
compression potential, which has been shown theoretically
\cite{maunz11efficient}. Empirical results confirmed the compression in
practice, while retaining good database coverage.  Moreover, it has been shown
that the structural constraints produce structurally diverse features with low
co-occurrence rates. BBRC descriptors compare favorable to other compressed
representations in the context of classification models.

Two isomorphic subgraphs will be always represented by the same string
identifier. We employ \emph{SMARTS}, a kind of regular expression to describe
molecular fragments for the strings. This allows to store results in a hash
structure.

When subgraph mining is finished over all bootstrap samples, a post-processing
step filters out infrequent (threshold from section \ref{ss:oob-dgm}) and
insignificant subgraphs (as assessed on the basis of the current method, see
section \ref{s:significance-estimation}). 

The algorithm using $numBoots=N$ bootstrap samples is shown in Algorithm
\ref{alg:bbrc-sample}.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
  \caption{Estimate subgraph significance on out-of-bag instances}
  \label{alg:bbrc-sample}
\begin{algorithmic}[1]
  \Require $dataBase, numBoots, minSamplingSupport, minFrequencyPerSample$
  \If{$numBoots=1$}
    \State $ans \gets BBRC(dataBase, minFrequencyPerSample)$
  \Else
    \State $hash \gets \{\}$
    \For{$i:=1 \to numBoots$} \Comment{Parallel Processing}
        \State $res \gets BBRC(drawBsSample(dataBase), minFrequencyPerSample)$
      \State $insert(hash,res)$
    \EndFor
    \State $ans \gets \emptyset$
    \For{$subgraph \in keys(hash)$}
      \If{$length(hash[subgraph]) \geq minSamplingSupport$}
      \If{$(p=SignificanceTest(hash[subgraph]))>1-\alpha$}
          \State $ans\gets ans \cup (subgraph,p)$
        \EndIf
      \EndIf
    \EndFor
  \EndIf
  \Ensure $ans$
\end{algorithmic}
\end{algorithm}

We consider the case where $numBoots>1$. Line 4 creates an initially empty hash
table to gather results from BBRC mining in line 6. The result
$res$ consists of subgraphs and support per class, i.e. each
subgraph is used as key in the hash, where the values stored are the
supports. Importantly, these values correspond to the
out-of-bag instances, not the in-bag instances (bootstrap samples). The
necessary step of matching the subgraphs on the out-of-bag instances happens
inside the BBRC step. On termination of the loop in line 8, each hash entry has
at most $N$ supports per class.

Post-processing the results is very fast and incurs negligible overhead
compared to the graph mining step. It consists of removing subgraphs that (line
11) were not generated often enough by the sampling process (have not enough
entries in the hash table, as determined by $minSamplingSupport$), or which
(line 12) do not significantly deviate from the overall distribution of classes,
as assessed by the appropriate $\chi^2$ test variant .

%For better readability, the listing does not show how the results in $ans$ are
%processed further: The caller of Algorithm \ref{alg:bbrc-sample} matches the
%subgraphs back onto the graphs of the original database ($G$), which yields an
%instantiation matrix, with compounds in the rows and subgraphs (molecular
%fragments) in the columns. This matching is again done in parallel, and matrix
%entries can either be of type binary (occurrence vs no occurrence) or frequency
%(how many times the subgraph occurs).

\section{Experiments}
\label{s:Experiments}
Experiments were conducted to assess the quality of the out-of-bag estimation.

\subsection{Comparison of $p$-Values} Three methods were compared by their
ability to estimate the discriminative potential of subgraphs, by assessing the
deviation between the $p$-values of subgraphs, as a) estimated by the
respective method, and b) obtained by matching the subgraphs to an independent
test set

The methods compared are

\begin{enumerate}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to section \ref{ss:MLE}. Denote this method by MLE.}
  \item{Out-of-bag estimation of $p$-values by computing chi-square values with Algorithm \ref{alg:bbrc-sample}, according to section \ref{ss:simple-mean}. Denote this method by MEAN.}
  \item{Single runs of the BBRC algorithm. Denote this method by BBRC.} 
\end{enumerate}

The process was repeated 100 times, with 50 bootstrap samples for methods 1 and 2. The whole procedure is described in Algorithm \ref{alg:pValEstimate}.
\begin{algorithm}
  \caption{Estimation of $p$-values}
  \label{alg:pValEstimate}
\begin{algorithmic}[1]
  \Require $graphDatabase, method$ \Comment{method is MLE, MEAN, or BBRC}
  \State $\mathbf{E_1}=\mathbf{E_2}=\left[ \right]$
  \For{$i:=1 \to 50$}
    \State $[trainSet, testSet] = splitStratified(graphDatabase,0.5)$ \Comment{Split 50:50}
    \State $\left[ \mathbf{subgraphs}, \mathbf{p^B} \right] = Algorithm \ref{alg:bbrc-sample}(trainSet,numBoots=100,\dots)$ \Comment{$numBoots=1$ for BBRC}
    \State $\mathbf{p'} = match(\mathbf{subgraphs}, test)$ 
    \State $ \mathbf{E_1} = \left[ \mathbf{E_1}, E_1(\mathbf{p'}, \mathbf{p^B}) \right]$
    \State $ \mathbf{E_2} = \left[ \mathbf{E_2}, E_2(\mathbf{p'}, \mathbf{p^B}) \right]$
  \EndFor
  \Ensure $\mathbf{E_1},\mathbf{E_2}$
\end{algorithmic}
\end{algorithm}

Line 1 initializes empty vectors that capture the residuals in
estimation. Inside the main loop, a stratified split (i.e. proportions of
target classes inside each split equal overall proportions) generates a training
and a test set of equal size. The training set is treated by the selected method,
which returns a vector of subgraphs and a vector of $p$-values, called
$\mathbf{p^B}$ (line 4). The subgraphs are matched on the test set, yielding
$p$-values $\mathbf{p'}$ (line 5). Finally, the residual vectors capture the
differences between $\mathbf{p^B}$ and  $\mathbf{p'}$ by two different metrics:

\begin{enumerate}
  \item $E_1 := \frac{1}{n} \sum_{i=1}^n \,p^B_i -p'_i \,$, to identify bias.
  \item $E_2 := \frac{1}{n} \sum_{i=1}^n \Big|\,p^B_i -p'_i \,\Big|$, to assess accuracy
\end{enumerate}

Table \ref{t:anal} details the results (mean values across bootstraps)
\input{anal}

Table \ref{t:sign} details the results ($n$=100). 
\input{sign}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp1.eps} & \includegraphics[width=4.4cm]{lp1.eps} \\
   \includegraphics[width=10cm]{bp2.eps} & \includegraphics[width=4.4cm]{lp2.eps} \\
   \includegraphics[width=10cm]{bp3.eps} & \includegraphics[width=4.4cm]{lp3.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp13}
\end{figure}

\begin{figure}[t]
  \begin{tabular}{cc}
   \includegraphics[width=10cm]{bp4.eps} & \includegraphics[width=4.4cm]{lp4.eps} \\
   \includegraphics[width=10cm]{bp5.eps} & \includegraphics[width=4.4cm]{lp5.eps} \\
  \end{tabular}
  \caption{Results}
  \label{fig:bplp45}
\end{figure}

Figure \ref{fig:bplp13} and Figure \ref{fig:bplp45} plot the results.

\section{Conclusion}
\label{s:Conclusion}


\bibliography{bbrc-sample}
\bibliographystyle{plain}

\end{document} 
